{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A custom Model\n",
    "This notebook shows how to implement and train a model other than the baseline used for this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import creating_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting val datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Getting train datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Number of moves: 70 | Dataset: valery_first_standart_elbow_left\n",
      "Reorder this dataset valery_first_standart_elbow_left True\n",
      "Number of moves: 135 | Dataset: alex_kovalev_standart_elbow_left\n",
      "Reorder this dataset alex_kovalev_standart_elbow_left True\n",
      "Number of moves: 72 | Dataset: anna_makarova_standart_elbow_left\n",
      "Reorder this dataset anna_makarova_standart_elbow_left True\n",
      "Number of moves: 62 | Dataset: artem_snailbox_standart_elbow_left\n",
      "Reorder this dataset artem_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: matthew_antonov_standart_elbow_left\n",
      "Reorder this dataset matthew_antonov_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_left\n",
      "Reorder this dataset misha_korobok_standart_elbow_left True\n",
      "Number of moves: 71 | Dataset: nikita_snailbox_standart_elbow_left\n",
      "Reorder this dataset nikita_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: petya_chizhov_standart_elbow_left\n",
      "Reorder this dataset petya_chizhov_standart_elbow_left True\n",
      "Number of moves: 12 | Dataset: polina_maksimova_standart_elbow_left\n",
      "Reorder this dataset polina_maksimova_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: sema_duplin_standart_elbow_left\n",
      "Reorder this dataset sema_duplin_standart_elbow_left True\n",
      "Number of moves: 136 | Dataset: alex_kovalev_standart_elbow_right\n",
      "Number of moves: 69 | Dataset: andrew_snailbox_standart_elbow_right\n",
      "Number of moves: 132 | Dataset: anna_makarova_standart_elbow_right\n",
      "Number of moves: 67 | Dataset: artem_snailbox_standart_elbow_right\n",
      "Number of moves: 68 | Dataset: matthew_antonov_standart_elbow_right\n",
      "Number of moves: 72 | Dataset: matvey_gorbenko_standart_elbow_right\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_right\n",
      "Number of moves: 55 | Dataset: nikita_snailbox_standart_elbow_right\n",
      "Number of moves: 142 | Dataset: petya_chizhov_standart_elbow_right\n",
      "Number of moves: 54 | Dataset: polina_maksimova_standart_elbow_right\n",
      "Number of moves: 139 | Dataset: sema_duplin_standart_elbow_right\n",
      "Number of trainining sessions: 22\n",
      "Number of validation sessions: 1\n",
      "Size of the input (8, 256) || Size of the output (20, 32)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"/msc/home/vsharm64/projects/BCI_Kaggle/dataset_v2_blocks/dataset_v2_blocks\"\n",
    "data_paths = dict(datasets=[DATA_PATH],\n",
    "                    hand_type = ['left', 'right'], # [left, 'right']\n",
    "                    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "                    test_dataset_list = ['fedya_tropin_standart_elbow_left'])\n",
    "data_config = creating_dataset.DataConfig(**data_paths)\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.decoder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import tqdm\n",
    "\n",
    "# class ConditionalTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "#         super(ConditionalTransformer, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         self.encoder_embedding = nn.Linear(input_dim, d_model)\n",
    "#         self.decoder_embedding = nn.Linear(output_dim, d_model)\n",
    "#         self.positional_encoding = nn.Parameter(torch.zeros(1, 5000, d_model))  # Positional encoding\n",
    "\n",
    "#         self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "#                                           num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "\n",
    "#         self.fc_out = nn.Linear(d_model, output_dim)\n",
    "#         self.downsample = nn.Sequential(\n",
    "#             nn.Linear(output_dim, output_dim // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(output_dim // 2, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, src, tgt):\n",
    "#         src_seq_len, tgt_seq_len = src.shape[1], tgt.shape[1]\n",
    "#         print(src.shape)\n",
    "#         src_emb = self.encoder_embedding(src) + self.positional_encoding[:, :src_seq_len, :]\n",
    "#         print(tgt.shape)\n",
    "#         tgt_emb = self.decoder_embedding(tgt) + self.positional_encoding[:, :tgt_seq_len, :]\n",
    "\n",
    "#         src_emb = src_emb.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "#         tgt_emb = tgt_emb.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "#         output = self.transformer(src_emb, tgt_emb)\n",
    "#         output = self.fc_out(output.permute(1, 0, 2))  # (batch_size, seq_len, output_dim)\n",
    "\n",
    "#         return self.downsample(output)\n",
    "\n",
    "# # Hyperparameters\n",
    "# input_dim = 8\n",
    "# output_dim = 20\n",
    "# d_model = 256\n",
    "# nhead = 8\n",
    "# num_encoder_layers = 4\n",
    "# num_decoder_layers = 4\n",
    "# dim_feedforward = 512\n",
    "# dropout = 0.1\n",
    "\n",
    "# # Initialize the model, criterion, and optimizer\n",
    "# model = ConditionalTransformer(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# n_epochs = 25\n",
    "\n",
    "# # Dummy train_dataset - replace with actual dataset\n",
    "# # Example shapes from your data\n",
    "# X_shape = (8, 256)\n",
    "# Y_shape = (20, 32)\n",
    "\n",
    "# # Generate dummy data for demonstration\n",
    "# # train_dataset = [(np.random.randn(*X_shape), np.random.randn(*Y_shape)) for _ in range(100)]\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     for X, Y in tqdm.tqdm(train_dataset):\n",
    "#         X, Y = torch.tensor(X.T).to(device), torch.tensor(Y).to(device)  # Transpose X to match expected input shape\n",
    "#         tgt = torch.zeros(Y.shape[0], Y.shape[1], dtype=torch.float32).to(device)  # Dummy target input for the transformer decoder\n",
    "#         print(tgt.shape)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         Y_hat = model(X.unsqueeze(0), tgt.unsqueeze(0)).squeeze(0)  # Add batch dimension\n",
    "#         loss = criterion(Y_hat, Y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "We define a simple Multi Layer Perceptron, fully connected feedforward neural network with 2 hidden layers. The model is implemented using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try making a prediction using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8, 256), Y shape: (20, 32)\n",
      "Predictions shape: (20, 256)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Predictions have the wrong shape!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m Y_hat \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mtensor(X\u001b[38;5;241m.\u001b[39mT))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY_hat\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m Y_hat\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions have the wrong shape!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Predictions have the wrong shape!"
     ]
    }
   ],
   "source": [
    "\n",
    "X, Y = train_dataset[0]\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "n_inputs, n_outputs = X.shape[0], Y.shape[0]\n",
    "n_hidden = 64\n",
    "\n",
    "model = MLP(n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "print(f\"Predictions shape: {Y_hat.shape}\")\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As upi cam see. The model's predictions are of the wrong shape: the outputs are not downsampled! Remember that inputs are sampled at 200Hz, but outputs are meant to be at 25Hz. We need to downsample the outputs to match the expected shape.\n",
    "\n",
    "You can choose your own downsampling method, but make sure that your predictions are aligned to the targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numpy.linalg import solve\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NTK:\n",
    "    def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.reg = reg\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.sol = None\n",
    "        self.Xtrain = None\n",
    "\n",
    "    def ntk_kernel(self, pair1, pair2):\n",
    "        out = pair1 @ pair2.T + 1\n",
    "        N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "        N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "        XX = np.sqrt(N1 @ N2.T)\n",
    "        out = out / XX\n",
    "\n",
    "        out = np.clip(out, -1, 1)\n",
    "\n",
    "        first = (\n",
    "            1\n",
    "            / np.pi\n",
    "            * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "            * XX\n",
    "        )\n",
    "        sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "        out = first + sec\n",
    "\n",
    "        C = 1\n",
    "        return out / C\n",
    "\n",
    "    def gaussian_kernel(self, pair1, pair2):\n",
    "        sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "        return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "    def kernel(self, pair1, pair2):\n",
    "        ntk = self.ntk_kernel(pair1, pair2)\n",
    "        gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "        return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "    def fit(self, Xtrain, ytrain):\n",
    "        K = self.kernel(Xtrain, Xtrain)\n",
    "        sol = solve(K + self.reg * np.eye(len(K)), ytrain).T\n",
    "        self.sol = sol\n",
    "        self.Xtrain = Xtrain\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        K = self.kernel(self.Xtrain, X)\n",
    "        return (self.sol @ K).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_downsample(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "solve: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (m,m),(m,n)->(m,n) (size 32 is different from 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Y_hat = model(torch.tensor(X.T)).detach().numpy().T\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Y_hat = simple_downsample(Y_hat)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model1 \u001b[38;5;241m=\u001b[39m NTK(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimple_downsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# model1.predict(X.T)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fit model and predict\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# model1 = model1.fit(torch.Tensor(X.T), train_out1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mNTK.fit\u001b[0;34m(self, Xtrain, ytrain)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, Xtrain, ytrain):\n\u001b[1;32m     50\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(Xtrain, Xtrain)\n\u001b[0;32m---> 51\u001b[0m     sol \u001b[38;5;241m=\u001b[39m \u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msol \u001b[38;5;241m=\u001b[39m sol\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXtrain \u001b[38;5;241m=\u001b[39m Xtrain\n",
      "File \u001b[0;32m~/miniconda3/envs/AiTLS/lib/python3.10/site-packages/numpy/linalg/linalg.py:409\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    407\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    408\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 409\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mValueError\u001b[0m: solve: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (m,m),(m,n)->(m,n) (size 32 is different from 256)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_downsample(x: np.ndarray) -> np.ndarray:\n",
    "    return x[:, ::data_config.down_sample_target]\n",
    "\n",
    "# Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "# Y_hat = simple_downsample(Y_hat)\n",
    "\n",
    "\n",
    "# assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\"\n",
    "\n",
    "model1 = NTK(reg=0.01,alpha=1.0)\n",
    "model1.fit(X.T, simple_downsample(Y.T))\n",
    "\n",
    "# model1.predict(X.T)\n",
    "# Fit model and predict\n",
    "# model1 = model1.fit(torch.Tensor(X.T), train_out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_downsample(x: np.ndarray) -> np.ndarray:\n",
    "    return x[:, ::data_config.down_sample_target]\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "Y_hat = simple_downsample(Y_hat)\n",
    "\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, you can build the downsampling into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x[::data_config.down_sample_target, :]\n",
    "    \n",
    "\n",
    "model = MLP(n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We define a very simple training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20065223.0544\n",
      "Epoch 2 Loss: 556769311.3171\n",
      "Epoch 3 Loss: 584892621.1726\n",
      "Epoch 4 Loss: 104045801.0032\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "lr = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(n_inputs, n_hidden, n_outputs).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for X,Y in train_dataset:\n",
    "        X, Y = torch.tensor(X.T).to(device), torch.tensor(Y).to(device)\n",
    "        Y_hat = model(X).T\n",
    "\n",
    "        loss = criterion(Y_hat, Y)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, Y = test_dataset[0]\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T).to(device)).detach().cpu().numpy().T\n",
    "\n",
    "f, axes = plt.subplots(5, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(Y[i], label=\"True\", lw=2, color='k')\n",
    "    ax.plot(Y_hat[i], label=\"Predicted\", lw=1, color='r')\n",
    "\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiTLS",
   "language": "python",
   "name": "aitls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
