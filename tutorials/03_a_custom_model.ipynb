{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A custom Model\n",
    "This notebook shows how to implement and train a model other than the baseline used for this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils import creating_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting val datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Getting train datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Number of moves: 70 | Dataset: valery_first_standart_elbow_left\n",
      "Reorder this dataset valery_first_standart_elbow_left True\n",
      "Number of moves: 135 | Dataset: alex_kovalev_standart_elbow_left\n",
      "Reorder this dataset alex_kovalev_standart_elbow_left True\n",
      "Number of moves: 72 | Dataset: anna_makarova_standart_elbow_left\n",
      "Reorder this dataset anna_makarova_standart_elbow_left True\n",
      "Number of moves: 62 | Dataset: artem_snailbox_standart_elbow_left\n",
      "Reorder this dataset artem_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: matthew_antonov_standart_elbow_left\n",
      "Reorder this dataset matthew_antonov_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_left\n",
      "Reorder this dataset misha_korobok_standart_elbow_left True\n",
      "Number of moves: 71 | Dataset: nikita_snailbox_standart_elbow_left\n",
      "Reorder this dataset nikita_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: petya_chizhov_standart_elbow_left\n",
      "Reorder this dataset petya_chizhov_standart_elbow_left True\n",
      "Number of moves: 12 | Dataset: polina_maksimova_standart_elbow_left\n",
      "Reorder this dataset polina_maksimova_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: sema_duplin_standart_elbow_left\n",
      "Reorder this dataset sema_duplin_standart_elbow_left True\n",
      "Number of moves: 136 | Dataset: alex_kovalev_standart_elbow_right\n",
      "Number of moves: 69 | Dataset: andrew_snailbox_standart_elbow_right\n",
      "Number of moves: 132 | Dataset: anna_makarova_standart_elbow_right\n",
      "Number of moves: 67 | Dataset: artem_snailbox_standart_elbow_right\n",
      "Number of moves: 68 | Dataset: matthew_antonov_standart_elbow_right\n",
      "Number of moves: 72 | Dataset: matvey_gorbenko_standart_elbow_right\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_right\n",
      "Number of moves: 55 | Dataset: nikita_snailbox_standart_elbow_right\n",
      "Number of moves: 142 | Dataset: petya_chizhov_standart_elbow_right\n",
      "Number of moves: 54 | Dataset: polina_maksimova_standart_elbow_right\n",
      "Number of moves: 139 | Dataset: sema_duplin_standart_elbow_right\n",
      "Number of trainining sessions: 22\n",
      "Number of validation sessions: 1\n",
      "Size of the input (8, 256) || Size of the output (20, 256)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"/msc/home/vsharm64/projects/BCI_Kaggle/dataset_v2_blocks/dataset_v2_blocks\"\n",
    "data_paths = dict(datasets=[DATA_PATH],\n",
    "                    hand_type = ['left', 'right'], # [left, 'right']\n",
    "                    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "                    test_dataset_list = ['fedya_tropin_standart_elbow_left'])\n",
    "# data_config = creating_dataset.DataConfig(**data_paths)\n",
    "data_config = creating_dataset.DataConfig(**data_paths,down_sample_target = 1)\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.decoder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import tqdm\n",
    "\n",
    "# class ConditionalTransformer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "#         super(ConditionalTransformer, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         self.encoder_embedding = nn.Linear(input_dim, d_model)\n",
    "#         self.decoder_embedding = nn.Linear(output_dim, d_model)\n",
    "#         self.positional_encoding = nn.Parameter(torch.zeros(1, 5000, d_model))  # Positional encoding\n",
    "\n",
    "#         self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "#                                           num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "\n",
    "#         self.fc_out = nn.Linear(d_model, output_dim)\n",
    "#         self.downsample = nn.Sequential(\n",
    "#             nn.Linear(output_dim, output_dim // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(output_dim // 2, output_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, src, tgt):\n",
    "#         src_seq_len, tgt_seq_len = src.shape[1], tgt.shape[1]\n",
    "#         print(src.shape)\n",
    "#         src_emb = self.encoder_embedding(src) + self.positional_encoding[:, :src_seq_len, :]\n",
    "#         print(tgt.shape)\n",
    "#         tgt_emb = self.decoder_embedding(tgt) + self.positional_encoding[:, :tgt_seq_len, :]\n",
    "\n",
    "#         src_emb = src_emb.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "#         tgt_emb = tgt_emb.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "#         output = self.transformer(src_emb, tgt_emb)\n",
    "#         output = self.fc_out(output.permute(1, 0, 2))  # (batch_size, seq_len, output_dim)\n",
    "\n",
    "#         return self.downsample(output)\n",
    "\n",
    "# # Hyperparameters\n",
    "# input_dim = 8\n",
    "# output_dim = 20\n",
    "# d_model = 256\n",
    "# nhead = 8\n",
    "# num_encoder_layers = 4\n",
    "# num_decoder_layers = 4\n",
    "# dim_feedforward = 512\n",
    "# dropout = 0.1\n",
    "\n",
    "# # Initialize the model, criterion, and optimizer\n",
    "# model = ConditionalTransformer(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# n_epochs = 25\n",
    "\n",
    "# # Dummy train_dataset - replace with actual dataset\n",
    "# # Example shapes from your data\n",
    "# X_shape = (8, 256)\n",
    "# Y_shape = (20, 32)\n",
    "\n",
    "# # Generate dummy data for demonstration\n",
    "# # train_dataset = [(np.random.randn(*X_shape), np.random.randn(*Y_shape)) for _ in range(100)]\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     for X, Y in tqdm.tqdm(train_dataset):\n",
    "#         X, Y = torch.tensor(X.T).to(device), torch.tensor(Y).to(device)  # Transpose X to match expected input shape\n",
    "#         tgt = torch.zeros(Y.shape[0], Y.shape[1], dtype=torch.float32).to(device)  # Dummy target input for the transformer decoder\n",
    "#         print(tgt.shape)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         Y_hat = model(X.unsqueeze(0), tgt.unsqueeze(0)).squeeze(0)  # Add batch dimension\n",
    "#         loss = criterion(Y_hat, Y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "We define a simple Multi Layer Perceptron, fully connected feedforward neural network with 2 hidden layers. The model is implemented using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try making a prediction using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8, 256), Y shape: (20, 256)\n",
      "Predictions shape: (20, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, Y = train_dataset[0]\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "n_inputs, n_outputs = X.shape[0], Y.shape[0]\n",
    "n_hidden = 64\n",
    "\n",
    "model = MLP(n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "print(f\"Predictions shape: {Y_hat.shape}\")\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As upi cam see. The model's predictions are of the wrong shape: the outputs are not downsampled! Remember that inputs are sampled at 200Hz, but outputs are meant to be at 25Hz. We need to downsample the outputs to match the expected shape.\n",
    "\n",
    "You can choose your own downsampling method, but make sure that your predictions are aligned to the targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from numpy.linalg import solve\n",
    "import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NTK:\n",
    "    def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "        self.reg = reg\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.sol = None\n",
    "        self.Xtrain = None\n",
    "        self.ytrain = None\n",
    "\n",
    "    def ntk_kernel(self, pair1, pair2):\n",
    "        out = pair1 @ pair2.T + 1\n",
    "        N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "        N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "        XX = np.sqrt(N1 @ N2.T)\n",
    "        out = out / XX\n",
    "\n",
    "        out = np.clip(out, -1, 1)\n",
    "\n",
    "        first = (\n",
    "            1\n",
    "            / np.pi\n",
    "            * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "            * XX\n",
    "        )\n",
    "        sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "        out = first + sec\n",
    "\n",
    "        C = 1\n",
    "        return out / C\n",
    "\n",
    "    def gaussian_kernel(self, pair1, pair2):\n",
    "        sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "        return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "    def kernel(self, pair1, pair2):\n",
    "        ntk = self.ntk_kernel(pair1, pair2)\n",
    "        gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "        return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "    def fit(self, Xtrain, ytrain, batch_size=1000, epochs=1):\n",
    "        import tqdm\n",
    "        num_samples = Xtrain.shape[0]\n",
    "        self.Xtrain = Xtrain\n",
    "        self.ytrain = ytrain\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for i in tqdm.tqdm(range(0, num_samples, batch_size)):\n",
    "                subset_indices = indices[i:i+batch_size]\n",
    "                Xtrain_subset = Xtrain[subset_indices]\n",
    "                ytrain_subset = ytrain[subset_indices]\n",
    "                \n",
    "                K = self.kernel(Xtrain_subset, Xtrain_subset)\n",
    "                sol = solve(K + self.reg * np.eye(len(K)), ytrain_subset).T\n",
    "                \n",
    "                if self.sol is None:\n",
    "                    self.sol = sol\n",
    "                    self.Xtrain_subset = Xtrain_subset\n",
    "                else:\n",
    "                    self.sol = np.vstack([self.sol, sol])\n",
    "                    self.Xtrain_subset = np.vstack([self.Xtrain_subset, Xtrain_subset])\n",
    "                    \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] completed\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=1000):\n",
    "        num_samples = X.shape[0]\n",
    "        num_train_samples = self.Xtrain_subset.shape[0]\n",
    "        K = np.zeros((num_train_samples, num_samples))\n",
    "        \n",
    "        for i in tqdm.tqdm(range(0, num_train_samples, batch_size)):\n",
    "            for j in range(0, num_samples, batch_size):\n",
    "                K[i:i+batch_size, j:j+batch_size] = self.kernel(self.Xtrain_subset[i:i+batch_size], X[j:j+batch_size])\n",
    "        \n",
    "        return (self.sol @ K).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numpy.linalg import solve\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NTK:\n",
    "    def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.reg = reg\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.sol = None\n",
    "        self.Xtrain = None\n",
    "\n",
    "    def ntk_kernel(self, pair1, pair2):\n",
    "        out = pair1 @ pair2.T + 1\n",
    "        N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "        N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "        XX = np.sqrt(N1 @ N2.T)\n",
    "        out = out / XX\n",
    "\n",
    "        out = np.clip(out, -1, 1)\n",
    "\n",
    "        first = (\n",
    "            1\n",
    "            / np.pi\n",
    "            * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "            * XX\n",
    "        )\n",
    "        sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "        out = first + sec\n",
    "\n",
    "        C = 1\n",
    "        return out / C\n",
    "\n",
    "    def gaussian_kernel(self, pair1, pair2):\n",
    "        sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "        return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "    def kernel(self, pair1, pair2):\n",
    "        ntk = self.ntk_kernel(pair1, pair2)\n",
    "        gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "        return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "    def fit(self, Xtrain, ytrain):\n",
    "        K = self.kernel(Xtrain, Xtrain)\n",
    "        sol = solve(K + self.reg * np.eye(len(K)), ytrain).T\n",
    "        self.sol = sol\n",
    "        self.Xtrain = Xtrain\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        K = self.kernel(self.Xtrain, X)\n",
    "        return (self.sol @ K).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_downsample(x: np.ndarray) -> np.ndarray:\n",
    "    return x[:, ::8]#data_config.down_sample_target]\n",
    "    \n",
    "simple_downsample(Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99990\n",
      "2\n",
      "(8, 256)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(train_dataset[0]))\n",
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming train_dataset is your list of tuples with two arrays\n",
    "# train_dataset = [(X, Y), (X, Y), ...]  # replace with your actual data\n",
    "\n",
    "# Split the dataset into input (X) and output (Y) components\n",
    "X = [sample[0] for sample in train_dataset]\n",
    "Y = [sample[1] for sample in train_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays and stack along a new axis\n",
    "X_stacked = np.array(X).transpose((0,2,1))\n",
    "X_stacked = X_stacked.reshape(-1, X_stacked.shape[2])\n",
    "Y_stacked = np.array(Y).transpose((0,2,1))\n",
    "Y_stacked = Y_stacked.reshape(-1, Y_stacked.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25597440, 8)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25597440, 20)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25597\n",
      "Number of validation samples: 25571843\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform the train/validation split with a 30% validation size\n",
    "train_input, val_input, train_output, val_output = train_test_split(X_stacked, Y_stacked, test_size=0.999, random_state=42)\n",
    "\n",
    "# Now you have your splits\n",
    "print(f\"Number of training samples: {len(train_input)}\")\n",
    "print(f\"Number of validation samples: {len(val_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25597, 8)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25597, 20)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1000)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[:1000].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output[:1000].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0476, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model1 = NTK(reg=0.01,alpha=1.0)\n",
    "model1 = model1.fit(train_input,#[:1000],\n",
    "                    train_output#[:1000]\n",
    "                   )\n",
    "\n",
    "Y_hat = model1.predict(train_input)#[:1000])\n",
    "# print(Y_hat)\n",
    "loss = criterion(\n",
    "    torch.from_numpy(Y_hat),\n",
    "    torch.from_numpy(train_output)#[:1000])\n",
    ")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 25597)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25597, 20)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799920, 256)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99990 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.61it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 228.01it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 39.01it/s]\n",
      "  0%|          | 3/99990 [00:00<1:23:47, 19.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.63it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 44.63it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.12it/s]\n",
      "  0%|          | 6/99990 [00:00<1:07:11, 24.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 291.90it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 138.11it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 297.87it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 315.62it/s]\n",
      "  0%|          | 10/99990 [00:00<53:47, 30.98it/s] \n",
      "100%|██████████| 1/1 [00:00<00:00, 302.68it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 317.49it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.98it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 85.47it/s]\n",
      "  0%|          | 14/99990 [00:00<1:02:05, 26.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.38it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.03it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 321.62it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 307.30it/s]\n",
      "  0%|          | 18/99990 [00:00<56:27, 29.51it/s]  \n",
      "100%|██████████| 1/1 [00:00<00:00, 311.82it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.24it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.67it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 199.86it/s]\n",
      "  0%|          | 22/99990 [00:00<58:37, 28.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 56.39it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 316.77it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 304.22it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 277.40it/s]\n",
      "  0%|          | 26/99990 [00:00<53:39, 31.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 137.71it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.09it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 309.82it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 317.80it/s]\n",
      "  0%|          | 30/99990 [00:01<53:55, 30.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 316.58it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.08it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.99it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 305.86it/s]\n",
      "  0%|          | 34/99990 [00:01<1:16:13, 21.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 65.52it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.83it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 317.58it/s]\n",
      "  0%|          | 37/99990 [00:01<1:25:09, 19.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 324.59it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 87.19it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 314.06it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.60it/s]\n",
      "  0%|          | 41/99990 [00:01<1:14:31, 22.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 317.34it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 177.39it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 312.47it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 315.31it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.90it/s]\u001b[A\n",
      "  0%|          | 46/99990 [00:01<1:18:14, 21.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 268.25it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.93it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 315.31it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\u001b[A\n",
      "  0%|          | 50/99990 [00:02<1:31:18, 18.24it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 50/99990 [00:02<1:16:08, 21.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model1 \u001b[38;5;241m=\u001b[39m NTK(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     11\u001b[0m model1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m.\u001b[39mT, Y\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m---> 12\u001b[0m Y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(Y_hat)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     15\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Y_hat),\n\u001b[1;32m     16\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     17\u001b[0m )\n",
      "Cell \u001b[0;32mIn[49], line 77\u001b[0m, in \u001b[0;36mNTK.predict\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXtrain_subset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_samples, batch_size)):\n\u001b[0;32m---> 77\u001b[0m         K[i:i\u001b[38;5;241m+\u001b[39mbatch_size, j:j\u001b[38;5;241m+\u001b[39mbatch_size] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXtrain_subset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m:\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msol \u001b[38;5;241m@\u001b[39m K)\u001b[38;5;241m.\u001b[39mT\n",
      "Cell \u001b[0;32mIn[49], line 47\u001b[0m, in \u001b[0;36mNTK.kernel\u001b[0;34m(self, pair1, pair2)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, pair1, pair2):\n\u001b[0;32m---> 47\u001b[0m     ntk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mntk_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     gaussian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaussian_kernel(pair1, pair2)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m ntk \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha) \u001b[38;5;241m*\u001b[39m gaussian\n",
      "Cell \u001b[0;32mIn[49], line 33\u001b[0m, in \u001b[0;36mNTK.ntk_kernel\u001b[0;34m(self, pair1, pair2)\u001b[0m\n\u001b[1;32m     26\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m/\u001b[39m XX\n\u001b[1;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m first \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;241m*\u001b[39m (out \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marccos(out)) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;241m*\u001b[39m XX\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m sec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m out \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marccos(out)) \u001b[38;5;241m*\u001b[39m XX\n\u001b[1;32m     37\u001b[0m out \u001b[38;5;241m=\u001b[39m first \u001b[38;5;241m+\u001b[39m sec\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "n_epochs = 2\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    # for X,Y in tqdm.tqdm(train_dataset):\n",
    "        # X, Y = torch.tensor(X.T).to(device), torch.tensor(Y).to(device)\n",
    "    model1 = NTK(reg=0.01,alpha=1.0)\n",
    "    model1 = model1.fit(train_input,train_output)#X.T, Y.T)\n",
    "    Y_hat = model1.predict(X.T)\n",
    "    # print(Y_hat)\n",
    "    loss = criterion(\n",
    "        torch.from_numpy(Y_hat),\n",
    "        torch.from_numpy(Y.T)\n",
    "    )\n",
    "    # loss.backward()\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "        # optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 256)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 13.57it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2560 is different from 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 89\u001b[0m, in \u001b[0;36mNTK.predict\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_samples, batch_size):\n\u001b[1;32m     87\u001b[0m         K[i:i\u001b[38;5;241m+\u001b[39mbatch_size, j:j\u001b[38;5;241m+\u001b[39mbatch_size] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXtrain_subset[i:i\u001b[38;5;241m+\u001b[39mbatch_size], X[j:j\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m)\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2560 is different from 256)"
     ]
    }
   ],
   "source": [
    "model1.predict(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 209.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 223.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 220.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 54.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 50.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 127.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 214.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 56.85it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2560 is different from 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m model1 \u001b[38;5;241m=\u001b[39m NTK(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     13\u001b[0m model1\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m.\u001b[39mT, Y\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;66;03m#simple_downsample(Y.T))\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fit model and predict\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# model1 = model1.fit(torch.Tensor(X.T), train_out1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 89\u001b[0m, in \u001b[0;36mNTK.predict\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_samples, batch_size):\n\u001b[1;32m     87\u001b[0m         K[i:i\u001b[38;5;241m+\u001b[39mbatch_size, j:j\u001b[38;5;241m+\u001b[39mbatch_size] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXtrain_subset[i:i\u001b[38;5;241m+\u001b[39mbatch_size], X[j:j\u001b[38;5;241m+\u001b[39mbatch_size])\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m)\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2560 is different from 256)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_downsample(x: np.ndarray) -> np.ndarray:\n",
    "    return x[:, ::8]#data_config.down_sample_target]\n",
    "\n",
    "# Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "# Y_hat = simple_downsample(Y_hat)\n",
    "\n",
    "\n",
    "# assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\"\n",
    "\n",
    "model1 = NTK(reg=0.01,alpha=1.0)\n",
    "model1.fit(X.T, Y.T)#simple_downsample(Y.T))\n",
    "\n",
    "model1.predict(X.T)\n",
    "# Fit model and predict\n",
    "# model1 = model1.fit(torch.Tensor(X.T), train_out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_downsample(x: np.ndarray) -> np.ndarray:\n",
    "    return x[:, ::data_config.down_sample_target]\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "Y_hat = simple_downsample(Y_hat)\n",
    "\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, you can build the downsampling into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x[::data_config.down_sample_target, :]\n",
    "    \n",
    "\n",
    "model = MLP(n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T)).detach().numpy().T\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We define a very simple training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 20065223.0544\n",
      "Epoch 2 Loss: 556769311.3171\n",
      "Epoch 3 Loss: 584892621.1726\n",
      "Epoch 4 Loss: 104045801.0032\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "lr = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(n_inputs, n_hidden, n_outputs).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for X,Y in train_dataset:\n",
    "        X, Y = torch.tensor(X.T).to(device), torch.tensor(Y).to(device)\n",
    "        Y_hat = model(X).T\n",
    "\n",
    "        loss = criterion(Y_hat, Y)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, Y = test_dataset[0]\n",
    "\n",
    "Y_hat = model(torch.tensor(X.T).to(device)).detach().cpu().numpy().T\n",
    "\n",
    "f, axes = plt.subplots(5, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(Y[i], label=\"True\", lw=2, color='k')\n",
    "    ax.plot(Y_hat[i], label=\"Predicted\", lw=1, color='r')\n",
    "\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiTLS",
   "language": "python",
   "name": "aitls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
