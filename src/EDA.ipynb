{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58952e8-8419-4afc-8430-12586c8f4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg ## for some reason this didn't import for eigsh without this line\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numpy.linalg import solve\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from MARBLE import plotting, preprocessing, dynamics, net, postprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1e5433-613d-4e35-bb4b-e832803bac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser('~')\n",
    "dat_dir = os.path.join(root_dir,'projects/BCI_Kaggle/dataset_v2_blocks/dataset_v2_blocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509bb1d3-9122-4f4a-840a-891c8d552973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "      <th>level_8</th>\n",
       "      <th>level_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.12941176470588234, 0.1450980392156862, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_vr</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, -1.0], [-0.03047309966664615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_angles</td>\n",
       "      <td>[[0.30133037585990835, -0.14699556169912562, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[1331098.9952673502, 1331099.041548939, 133109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0121.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.11372549019607847, -0.027450980392156876,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             level_0            level_1 level_2 level_3  \\\n",
       "0  dataset_v2_blocks  dataset_v2_blocks  health    left   \n",
       "1  dataset_v2_blocks  dataset_v2_blocks  health    left   \n",
       "2  dataset_v2_blocks  dataset_v2_blocks  health    left   \n",
       "3  dataset_v2_blocks  dataset_v2_blocks  health    left   \n",
       "4  dataset_v2_blocks  dataset_v2_blocks  health    left   \n",
       "\n",
       "                            level_4         level_5 level_6   level_7  \\\n",
       "0  alex_kovalev_standart_elbow_left  preproc_angles   train  0037.npz   \n",
       "1  alex_kovalev_standart_elbow_left  preproc_angles   train  0037.npz   \n",
       "2  alex_kovalev_standart_elbow_left  preproc_angles   train  0037.npz   \n",
       "3  alex_kovalev_standart_elbow_left  preproc_angles   train  0037.npz   \n",
       "4  alex_kovalev_standart_elbow_left  preproc_angles   train  0121.npz   \n",
       "\n",
       "       level_8                                            level_9  \n",
       "0     data_myo  [[-0.12941176470588234, 0.1450980392156862, 0....  \n",
       "1      data_vr  [[[0.0, 0.0, 0.0, -1.0], [-0.03047309966664615...  \n",
       "2  data_angles  [[0.30133037585990835, -0.14699556169912562, 0...  \n",
       "3       myo_ts  [1331098.9952673502, 1331099.041548939, 133109...  \n",
       "4     data_myo  [[-0.11372549019607847, -0.027450980392156876,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_metadata(data_dict, base_key=[], metadata_rows=None):\n",
    "    if metadata_rows is None:\n",
    "        metadata_rows = []\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        current_key = base_key + [key]\n",
    "        if isinstance(value, dict):\n",
    "            extract_metadata(value, current_key, metadata_rows)\n",
    "        else:\n",
    "            # If value is not a dict, we are at the file level\n",
    "            row = dict(zip(range(len(current_key)), current_key))\n",
    "            row[len(current_key)] = value\n",
    "            metadata_rows.append(row)\n",
    "\n",
    "    return metadata_rows\n",
    "\n",
    "def load_npz_files(base_directory):\n",
    "    data_dict = {}\n",
    "\n",
    "    for root, _, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.npz'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extract metadata from file path\n",
    "                path_parts = file_path.split(os.sep)\n",
    "                # Get the relevant part of the path\n",
    "                relevant_parts = path_parts[-8:]\n",
    "                \n",
    "                # Load npz file\n",
    "                data = np.load(file_path)\n",
    "                data_dict_entry = {key: data[key] for key in data.files}\n",
    "                \n",
    "                # Construct the hierarchical dictionary\n",
    "                current_dict = data_dict\n",
    "                for part in relevant_parts[:-1]:\n",
    "                    if part not in current_dict:\n",
    "                        current_dict[part] = {}\n",
    "                    current_dict = current_dict[part]\n",
    "                \n",
    "                current_dict[relevant_parts[-1]] = data_dict_entry\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "base_directory = \"/msc/home/vsharm64/projects/BCI_Kaggle/dataset_v2_blocks/dataset_v2_blocks\"\n",
    "data_dict = load_npz_files(base_directory)\n",
    "\n",
    "# Extract metadata\n",
    "metadata_rows = extract_metadata(data_dict)\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "metadata_df = pd.DataFrame(metadata_rows).sort_index(axis=1)\n",
    "\n",
    "# Rename columns to more meaningful names\n",
    "metadata_df.columns = [f'level_{i}' for i in metadata_df.columns]\n",
    "\n",
    "# Display the DataFrame\n",
    "metadata_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94283e22-73d8-4617-8ca5-9046796f89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>matthew_antonov_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0006.npz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12404</td>\n",
       "      <td>12404</td>\n",
       "      <td>11128</td>\n",
       "      <td>6716</td>\n",
       "      <td>864</td>\n",
       "      <td>12404</td>\n",
       "      <td>8592</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  level_0            level_1 level_2 level_3  \\\n",
       "count               12404              12404   12404   12404   \n",
       "unique                  1                  1       2       2   \n",
       "top     dataset_v2_blocks  dataset_v2_blocks  health    left   \n",
       "freq                12404              12404   11128    6716   \n",
       "\n",
       "                                    level_4         level_5 level_6   level_7  \n",
       "count                                 12404           12404   12404     12404  \n",
       "unique                                   22               1       4       144  \n",
       "top     matthew_antonov_standart_elbow_left  preproc_angles   train  0006.npz  \n",
       "freq                                    864           12404    8592       138  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.iloc[:,:8].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c088ca0-9eda-4d2f-9dde-bc90ef76dfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'test', 'submit', 'train_strong_activity'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['level_6'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd675b2-b9b4-465a-9ad2-f1e0c572eb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "      <th>level_8</th>\n",
       "      <th>level_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11704</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.19215686274509802, 0.0039215686274509665, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11705</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[337481.88911415, 337481.93570765737, 337481.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11706</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0030.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.06666666666666665, -0.05882352941176472, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11707</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0030.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[337239.4001226501, 337239.4464885098, 337239....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11708</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0053.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.21568627450980393, -0.0039215686274509665...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11843</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0020.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[336939.7717833501, 336939.8205107955, 336939....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0039.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.0039215686274509665, 0.003921568627450966...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0039.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[337558.88148415, 337558.929068199, 337558.944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.0039215686274509665, -0.003921568627450966...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>fedya_tropin_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>submit</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[338181.78083390003, 338181.8261038943, 338181...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0            level_1   level_2 level_3  \\\n",
       "11704  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11705  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11706  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11707  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11708  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "...                  ...                ...       ...     ...   \n",
       "11843  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11844  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11845  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11846  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "11847  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "\n",
       "                                level_4         level_5 level_6   level_7  \\\n",
       "11704  fedya_tropin_standart_elbow_left  preproc_angles  submit  0037.npz   \n",
       "11705  fedya_tropin_standart_elbow_left  preproc_angles  submit  0037.npz   \n",
       "11706  fedya_tropin_standart_elbow_left  preproc_angles  submit  0030.npz   \n",
       "11707  fedya_tropin_standart_elbow_left  preproc_angles  submit  0030.npz   \n",
       "11708  fedya_tropin_standart_elbow_left  preproc_angles  submit  0053.npz   \n",
       "...                                 ...             ...     ...       ...   \n",
       "11843  fedya_tropin_standart_elbow_left  preproc_angles  submit  0020.npz   \n",
       "11844  fedya_tropin_standart_elbow_left  preproc_angles  submit  0039.npz   \n",
       "11845  fedya_tropin_standart_elbow_left  preproc_angles  submit  0039.npz   \n",
       "11846  fedya_tropin_standart_elbow_left  preproc_angles  submit  0055.npz   \n",
       "11847  fedya_tropin_standart_elbow_left  preproc_angles  submit  0055.npz   \n",
       "\n",
       "        level_8                                            level_9  \n",
       "11704  data_myo  [[0.19215686274509802, 0.0039215686274509665, ...  \n",
       "11705    myo_ts  [337481.88911415, 337481.93570765737, 337481.9...  \n",
       "11706  data_myo  [[-0.06666666666666665, -0.05882352941176472, ...  \n",
       "11707    myo_ts  [337239.4001226501, 337239.4464885098, 337239....  \n",
       "11708  data_myo  [[-0.21568627450980393, -0.0039215686274509665...  \n",
       "...         ...                                                ...  \n",
       "11843    myo_ts  [336939.7717833501, 336939.8205107955, 336939....  \n",
       "11844  data_myo  [[-0.0039215686274509665, 0.003921568627450966...  \n",
       "11845    myo_ts  [337558.88148415, 337558.929068199, 337558.944...  \n",
       "11846  data_myo  [[0.0039215686274509665, -0.003921568627450966...  \n",
       "11847    myo_ts  [338181.78083390003, 338181.8261038943, 338181...  \n",
       "\n",
       "[144 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.loc[metadata_df['level_6']=='submit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6dc143-1ffc-4639-9bce-0299dc004ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "      <th>level_8</th>\n",
       "      <th>level_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.12941176470588234, 0.1450980392156862, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_vr</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, -1.0], [-0.03047309966664615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_angles</td>\n",
       "      <td>[[0.30133037585990835, -0.14699556169912562, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[1331098.9952673502, 1331099.041548939, 133109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0121.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.11372549019607847, -0.027450980392156876,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0039.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[1339066.40039925, 1339066.4488972398, 1339066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.1843137254901961, -0.03529411764705881, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_vr</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, -1.0], [-0.05675145483850774...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12402</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_angles</td>\n",
       "      <td>[[-0.7271111805441305, -0.012663524826067493, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12403</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[1339671.20793555, 1339671.25543303, 1339671.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12404 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0            level_1   level_2 level_3  \\\n",
       "0      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "1      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "2      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "3      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "4      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "...                  ...                ...       ...     ...   \n",
       "12399  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12400  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12401  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12402  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12403  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "\n",
       "                                level_4         level_5  \\\n",
       "0      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "1      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "2      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "3      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "4      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "...                                 ...             ...   \n",
       "12399  valery_first_standart_elbow_left  preproc_angles   \n",
       "12400  valery_first_standart_elbow_left  preproc_angles   \n",
       "12401  valery_first_standart_elbow_left  preproc_angles   \n",
       "12402  valery_first_standart_elbow_left  preproc_angles   \n",
       "12403  valery_first_standart_elbow_left  preproc_angles   \n",
       "\n",
       "                     level_6   level_7      level_8  \\\n",
       "0                      train  0037.npz     data_myo   \n",
       "1                      train  0037.npz      data_vr   \n",
       "2                      train  0037.npz  data_angles   \n",
       "3                      train  0037.npz       myo_ts   \n",
       "4                      train  0121.npz     data_myo   \n",
       "...                      ...       ...          ...   \n",
       "12399  train_strong_activity  0039.npz       myo_ts   \n",
       "12400  train_strong_activity  0055.npz     data_myo   \n",
       "12401  train_strong_activity  0055.npz      data_vr   \n",
       "12402  train_strong_activity  0055.npz  data_angles   \n",
       "12403  train_strong_activity  0055.npz       myo_ts   \n",
       "\n",
       "                                                 level_9  \n",
       "0      [[-0.12941176470588234, 0.1450980392156862, 0....  \n",
       "1      [[[0.0, 0.0, 0.0, -1.0], [-0.03047309966664615...  \n",
       "2      [[0.30133037585990835, -0.14699556169912562, 0...  \n",
       "3      [1331098.9952673502, 1331099.041548939, 133109...  \n",
       "4      [[-0.11372549019607847, -0.027450980392156876,...  \n",
       "...                                                  ...  \n",
       "12399  [1339066.40039925, 1339066.4488972398, 1339066...  \n",
       "12400  [[0.1843137254901961, -0.03529411764705881, 0....  \n",
       "12401  [[[0.0, 0.0, 0.0, -1.0], [-0.05675145483850774...  \n",
       "12402  [[-0.7271111805441305, -0.012663524826067493, ...  \n",
       "12403  [1339671.20793555, 1339671.25543303, 1339671.2...  \n",
       "\n",
       "[12404 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df#.loc[metadata_df['level_6']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d59606c-2935-4313-b504-f2450a443835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.iloc[0]['level_9'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bc03b2-715f-4b62-adcd-7c9821cc913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_onehot_cols = ['level_2','level_3','level_4']#,'level_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ce2180-b671-4430-b66d-817a0ad4e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_2_amputant</th>\n",
       "      <th>level_2_health</th>\n",
       "      <th>level_3_left</th>\n",
       "      <th>level_3_right</th>\n",
       "      <th>level_4_alex_kovalev_standart_elbow_left</th>\n",
       "      <th>level_4_alex_kovalev_standart_elbow_right</th>\n",
       "      <th>level_4_andrew_snailbox_standart_elbow_right</th>\n",
       "      <th>level_4_anna_makarova_standart_elbow_left</th>\n",
       "      <th>level_4_anna_makarova_standart_elbow_right</th>\n",
       "      <th>level_4_artem_snailbox_standart_elbow_left</th>\n",
       "      <th>...</th>\n",
       "      <th>level_4_misha_korobok_standart_elbow_right</th>\n",
       "      <th>level_4_nikita_snailbox_standart_elbow_left</th>\n",
       "      <th>level_4_nikita_snailbox_standart_elbow_right</th>\n",
       "      <th>level_4_petya_chizhov_standart_elbow_left</th>\n",
       "      <th>level_4_petya_chizhov_standart_elbow_right</th>\n",
       "      <th>level_4_polina_maksimova_standart_elbow_left</th>\n",
       "      <th>level_4_polina_maksimova_standart_elbow_right</th>\n",
       "      <th>level_4_sema_duplin_standart_elbow_left</th>\n",
       "      <th>level_4_sema_duplin_standart_elbow_right</th>\n",
       "      <th>level_4_valery_first_standart_elbow_left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12402</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12403</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12404 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_2_amputant  level_2_health  level_3_left  level_3_right  \\\n",
       "0                 False            True          True          False   \n",
       "1                 False            True          True          False   \n",
       "2                 False            True          True          False   \n",
       "3                 False            True          True          False   \n",
       "4                 False            True          True          False   \n",
       "...                 ...             ...           ...            ...   \n",
       "12399              True           False          True          False   \n",
       "12400              True           False          True          False   \n",
       "12401              True           False          True          False   \n",
       "12402              True           False          True          False   \n",
       "12403              True           False          True          False   \n",
       "\n",
       "       level_4_alex_kovalev_standart_elbow_left  \\\n",
       "0                                          True   \n",
       "1                                          True   \n",
       "2                                          True   \n",
       "3                                          True   \n",
       "4                                          True   \n",
       "...                                         ...   \n",
       "12399                                     False   \n",
       "12400                                     False   \n",
       "12401                                     False   \n",
       "12402                                     False   \n",
       "12403                                     False   \n",
       "\n",
       "       level_4_alex_kovalev_standart_elbow_right  \\\n",
       "0                                          False   \n",
       "1                                          False   \n",
       "2                                          False   \n",
       "3                                          False   \n",
       "4                                          False   \n",
       "...                                          ...   \n",
       "12399                                      False   \n",
       "12400                                      False   \n",
       "12401                                      False   \n",
       "12402                                      False   \n",
       "12403                                      False   \n",
       "\n",
       "       level_4_andrew_snailbox_standart_elbow_right  \\\n",
       "0                                             False   \n",
       "1                                             False   \n",
       "2                                             False   \n",
       "3                                             False   \n",
       "4                                             False   \n",
       "...                                             ...   \n",
       "12399                                         False   \n",
       "12400                                         False   \n",
       "12401                                         False   \n",
       "12402                                         False   \n",
       "12403                                         False   \n",
       "\n",
       "       level_4_anna_makarova_standart_elbow_left  \\\n",
       "0                                          False   \n",
       "1                                          False   \n",
       "2                                          False   \n",
       "3                                          False   \n",
       "4                                          False   \n",
       "...                                          ...   \n",
       "12399                                      False   \n",
       "12400                                      False   \n",
       "12401                                      False   \n",
       "12402                                      False   \n",
       "12403                                      False   \n",
       "\n",
       "       level_4_anna_makarova_standart_elbow_right  \\\n",
       "0                                           False   \n",
       "1                                           False   \n",
       "2                                           False   \n",
       "3                                           False   \n",
       "4                                           False   \n",
       "...                                           ...   \n",
       "12399                                       False   \n",
       "12400                                       False   \n",
       "12401                                       False   \n",
       "12402                                       False   \n",
       "12403                                       False   \n",
       "\n",
       "       level_4_artem_snailbox_standart_elbow_left  ...  \\\n",
       "0                                           False  ...   \n",
       "1                                           False  ...   \n",
       "2                                           False  ...   \n",
       "3                                           False  ...   \n",
       "4                                           False  ...   \n",
       "...                                           ...  ...   \n",
       "12399                                       False  ...   \n",
       "12400                                       False  ...   \n",
       "12401                                       False  ...   \n",
       "12402                                       False  ...   \n",
       "12403                                       False  ...   \n",
       "\n",
       "       level_4_misha_korobok_standart_elbow_right  \\\n",
       "0                                           False   \n",
       "1                                           False   \n",
       "2                                           False   \n",
       "3                                           False   \n",
       "4                                           False   \n",
       "...                                           ...   \n",
       "12399                                       False   \n",
       "12400                                       False   \n",
       "12401                                       False   \n",
       "12402                                       False   \n",
       "12403                                       False   \n",
       "\n",
       "       level_4_nikita_snailbox_standart_elbow_left  \\\n",
       "0                                            False   \n",
       "1                                            False   \n",
       "2                                            False   \n",
       "3                                            False   \n",
       "4                                            False   \n",
       "...                                            ...   \n",
       "12399                                        False   \n",
       "12400                                        False   \n",
       "12401                                        False   \n",
       "12402                                        False   \n",
       "12403                                        False   \n",
       "\n",
       "       level_4_nikita_snailbox_standart_elbow_right  \\\n",
       "0                                             False   \n",
       "1                                             False   \n",
       "2                                             False   \n",
       "3                                             False   \n",
       "4                                             False   \n",
       "...                                             ...   \n",
       "12399                                         False   \n",
       "12400                                         False   \n",
       "12401                                         False   \n",
       "12402                                         False   \n",
       "12403                                         False   \n",
       "\n",
       "       level_4_petya_chizhov_standart_elbow_left  \\\n",
       "0                                          False   \n",
       "1                                          False   \n",
       "2                                          False   \n",
       "3                                          False   \n",
       "4                                          False   \n",
       "...                                          ...   \n",
       "12399                                      False   \n",
       "12400                                      False   \n",
       "12401                                      False   \n",
       "12402                                      False   \n",
       "12403                                      False   \n",
       "\n",
       "       level_4_petya_chizhov_standart_elbow_right  \\\n",
       "0                                           False   \n",
       "1                                           False   \n",
       "2                                           False   \n",
       "3                                           False   \n",
       "4                                           False   \n",
       "...                                           ...   \n",
       "12399                                       False   \n",
       "12400                                       False   \n",
       "12401                                       False   \n",
       "12402                                       False   \n",
       "12403                                       False   \n",
       "\n",
       "       level_4_polina_maksimova_standart_elbow_left  \\\n",
       "0                                             False   \n",
       "1                                             False   \n",
       "2                                             False   \n",
       "3                                             False   \n",
       "4                                             False   \n",
       "...                                             ...   \n",
       "12399                                         False   \n",
       "12400                                         False   \n",
       "12401                                         False   \n",
       "12402                                         False   \n",
       "12403                                         False   \n",
       "\n",
       "       level_4_polina_maksimova_standart_elbow_right  \\\n",
       "0                                              False   \n",
       "1                                              False   \n",
       "2                                              False   \n",
       "3                                              False   \n",
       "4                                              False   \n",
       "...                                              ...   \n",
       "12399                                          False   \n",
       "12400                                          False   \n",
       "12401                                          False   \n",
       "12402                                          False   \n",
       "12403                                          False   \n",
       "\n",
       "       level_4_sema_duplin_standart_elbow_left  \\\n",
       "0                                        False   \n",
       "1                                        False   \n",
       "2                                        False   \n",
       "3                                        False   \n",
       "4                                        False   \n",
       "...                                        ...   \n",
       "12399                                    False   \n",
       "12400                                    False   \n",
       "12401                                    False   \n",
       "12402                                    False   \n",
       "12403                                    False   \n",
       "\n",
       "       level_4_sema_duplin_standart_elbow_right  \\\n",
       "0                                         False   \n",
       "1                                         False   \n",
       "2                                         False   \n",
       "3                                         False   \n",
       "4                                         False   \n",
       "...                                         ...   \n",
       "12399                                     False   \n",
       "12400                                     False   \n",
       "12401                                     False   \n",
       "12402                                     False   \n",
       "12403                                     False   \n",
       "\n",
       "       level_4_valery_first_standart_elbow_left  \n",
       "0                                         False  \n",
       "1                                         False  \n",
       "2                                         False  \n",
       "3                                         False  \n",
       "4                                         False  \n",
       "...                                         ...  \n",
       "12399                                      True  \n",
       "12400                                      True  \n",
       "12401                                      True  \n",
       "12402                                      True  \n",
       "12403                                      True  \n",
       "\n",
       "[12404 rows x 26 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform one-hot encoding\n",
    "onehot_encoded_df = pd.get_dummies(metadata_df[to_onehot_cols])\n",
    "onehot_encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e895e6b8-99fa-435f-b779-7e01c1bcb0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "      <th>level_8</th>\n",
       "      <th>level_9</th>\n",
       "      <th>OneHot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.12941176470588234, 0.1450980392156862, 0....</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_vr</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, -1.0], [-0.03047309966664615...</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_angles</td>\n",
       "      <td>[[0.30133037585990835, -0.14699556169912562, 0...</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[1331098.9952673502, 1331099.041548939, 133109...</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0121.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.11372549019607847, -0.027450980392156876,...</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12399</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0039.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[1339066.40039925, 1339066.4488972398, 1339066...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.1843137254901961, -0.03529411764705881, 0....</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12401</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_vr</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, -1.0], [-0.05675145483850774...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12402</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_angles</td>\n",
       "      <td>[[-0.7271111805441305, -0.012663524826067493, ...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12403</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train_strong_activity</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>myo_ts</td>\n",
       "      <td>[1339671.20793555, 1339671.25543303, 1339671.2...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12404 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0            level_1   level_2 level_3  \\\n",
       "0      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "1      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "2      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "3      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "4      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "...                  ...                ...       ...     ...   \n",
       "12399  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12400  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12401  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12402  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12403  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "\n",
       "                                level_4         level_5  \\\n",
       "0      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "1      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "2      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "3      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "4      alex_kovalev_standart_elbow_left  preproc_angles   \n",
       "...                                 ...             ...   \n",
       "12399  valery_first_standart_elbow_left  preproc_angles   \n",
       "12400  valery_first_standart_elbow_left  preproc_angles   \n",
       "12401  valery_first_standart_elbow_left  preproc_angles   \n",
       "12402  valery_first_standart_elbow_left  preproc_angles   \n",
       "12403  valery_first_standart_elbow_left  preproc_angles   \n",
       "\n",
       "                     level_6   level_7      level_8  \\\n",
       "0                      train  0037.npz     data_myo   \n",
       "1                      train  0037.npz      data_vr   \n",
       "2                      train  0037.npz  data_angles   \n",
       "3                      train  0037.npz       myo_ts   \n",
       "4                      train  0121.npz     data_myo   \n",
       "...                      ...       ...          ...   \n",
       "12399  train_strong_activity  0039.npz       myo_ts   \n",
       "12400  train_strong_activity  0055.npz     data_myo   \n",
       "12401  train_strong_activity  0055.npz      data_vr   \n",
       "12402  train_strong_activity  0055.npz  data_angles   \n",
       "12403  train_strong_activity  0055.npz       myo_ts   \n",
       "\n",
       "                                                 level_9  \\\n",
       "0      [[-0.12941176470588234, 0.1450980392156862, 0....   \n",
       "1      [[[0.0, 0.0, 0.0, -1.0], [-0.03047309966664615...   \n",
       "2      [[0.30133037585990835, -0.14699556169912562, 0...   \n",
       "3      [1331098.9952673502, 1331099.041548939, 133109...   \n",
       "4      [[-0.11372549019607847, -0.027450980392156876,...   \n",
       "...                                                  ...   \n",
       "12399  [1339066.40039925, 1339066.4488972398, 1339066...   \n",
       "12400  [[0.1843137254901961, -0.03529411764705881, 0....   \n",
       "12401  [[[0.0, 0.0, 0.0, -1.0], [-0.05675145483850774...   \n",
       "12402  [[-0.7271111805441305, -0.012663524826067493, ...   \n",
       "12403  [1339671.20793555, 1339671.25543303, 1339671.2...   \n",
       "\n",
       "                                                  OneHot  \n",
       "0      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                  ...  \n",
       "12399  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12400  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12401  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12402  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12403  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[12404 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['OneHot'] = onehot_encoded_df.apply(lambda row: [np.array(row,dtype=float)],axis=1)\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970f9bfd-fdfc-4ab1-8850-17b54b1ef6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = metadata_df.loc[metadata_df['level_6']=='train']\n",
    "test_subset = metadata_df.loc[metadata_df['level_6']=='test']\n",
    "submit_subset = metadata_df.loc[metadata_df['level_6']=='submit']\n",
    "\n",
    "train_inputs = train_subset.loc[train_subset['level_8']=='data_myo']\n",
    "train_outputs = train_subset.loc[train_subset['level_8']=='data_angles']\n",
    "\n",
    "test_inputs = test_subset.loc[test_subset['level_8']=='data_myo']\n",
    "test_outputs = test_subset.loc[test_subset['level_8']=='data_angles']\n",
    "\n",
    "submit_inputs = submit_subset.loc[submit_subset['level_8']=='data_myo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f24677-0ad5-4f76-863d-fce67ccc5481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_3</th>\n",
       "      <th>level_4</th>\n",
       "      <th>level_5</th>\n",
       "      <th>level_6</th>\n",
       "      <th>level_7</th>\n",
       "      <th>level_8</th>\n",
       "      <th>level_9</th>\n",
       "      <th>OneHot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0037.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.12941176470588234, 0.1450980392156862, 0....</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0121.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.11372549019607847, -0.027450980392156876,...</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0030.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.2313725490196078, -0.05882352941176472, -...</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0053.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.10588235294117654, 0.027450980392156765, -...</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>health</td>\n",
       "      <td>left</td>\n",
       "      <td>alex_kovalev_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0025.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.05882352941176472, 0.03529411764705892, 0....</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12108</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0033.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.05882352941176472, 0.027450980392156765, -...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12112</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0068.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[-0.13725490196078427, -0.1686274509803921, -...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0020.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.0039215686274509665, 0.08235294117647052, ...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12120</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0039.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.09019607843137245, 0.050980392156862786, 0...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12124</th>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>dataset_v2_blocks</td>\n",
       "      <td>amputant</td>\n",
       "      <td>left</td>\n",
       "      <td>valery_first_standart_elbow_left</td>\n",
       "      <td>preproc_angles</td>\n",
       "      <td>train</td>\n",
       "      <td>0055.npz</td>\n",
       "      <td>data_myo</td>\n",
       "      <td>[[0.15294117647058814, -0.07450980392156858, -...</td>\n",
       "      <td>[[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2148 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 level_0            level_1   level_2 level_3  \\\n",
       "0      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "4      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "8      dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "12     dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "16     dataset_v2_blocks  dataset_v2_blocks    health    left   \n",
       "...                  ...                ...       ...     ...   \n",
       "12108  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12112  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12116  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12120  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "12124  dataset_v2_blocks  dataset_v2_blocks  amputant    left   \n",
       "\n",
       "                                level_4         level_5 level_6   level_7  \\\n",
       "0      alex_kovalev_standart_elbow_left  preproc_angles   train  0037.npz   \n",
       "4      alex_kovalev_standart_elbow_left  preproc_angles   train  0121.npz   \n",
       "8      alex_kovalev_standart_elbow_left  preproc_angles   train  0030.npz   \n",
       "12     alex_kovalev_standart_elbow_left  preproc_angles   train  0053.npz   \n",
       "16     alex_kovalev_standart_elbow_left  preproc_angles   train  0025.npz   \n",
       "...                                 ...             ...     ...       ...   \n",
       "12108  valery_first_standart_elbow_left  preproc_angles   train  0033.npz   \n",
       "12112  valery_first_standart_elbow_left  preproc_angles   train  0068.npz   \n",
       "12116  valery_first_standart_elbow_left  preproc_angles   train  0020.npz   \n",
       "12120  valery_first_standart_elbow_left  preproc_angles   train  0039.npz   \n",
       "12124  valery_first_standart_elbow_left  preproc_angles   train  0055.npz   \n",
       "\n",
       "        level_8                                            level_9  \\\n",
       "0      data_myo  [[-0.12941176470588234, 0.1450980392156862, 0....   \n",
       "4      data_myo  [[-0.11372549019607847, -0.027450980392156876,...   \n",
       "8      data_myo  [[-0.2313725490196078, -0.05882352941176472, -...   \n",
       "12     data_myo  [[0.10588235294117654, 0.027450980392156765, -...   \n",
       "16     data_myo  [[0.05882352941176472, 0.03529411764705892, 0....   \n",
       "...         ...                                                ...   \n",
       "12108  data_myo  [[0.05882352941176472, 0.027450980392156765, -...   \n",
       "12112  data_myo  [[-0.13725490196078427, -0.1686274509803921, -...   \n",
       "12116  data_myo  [[0.0039215686274509665, 0.08235294117647052, ...   \n",
       "12120  data_myo  [[0.09019607843137245, 0.050980392156862786, 0...   \n",
       "12124  data_myo  [[0.15294117647058814, -0.07450980392156858, -...   \n",
       "\n",
       "                                                  OneHot  \n",
       "0      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "8      [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12     [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "16     [[0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                  ...  \n",
       "12108  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12112  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12116  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12120  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "12124  [[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[2148 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4261cf1-08a8-4b9e-b237-4985b93257f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3722, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.iloc[0]['level_9'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "294923d0-4464-457a-a94a-00b34a5b799d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.iloc[0]['OneHot'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14de35f4-645e-4063-afd5-ee016ca7ea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2894668/1065248138.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_inputs['combined'] = pd.Series(train_inputs.apply(expand_and_concatenate, axis=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3722, 34)\n",
      "(3729, 34)\n",
      "(3725, 34)\n",
      "[[-0.12941176  0.14509804  0.06666667 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17647059  0.00392157 -0.01176471 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.16862745 -0.05098039 -0.0745098  ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.00392157 -0.01176471 -0.02745098 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.15294118 -0.01960784 -0.01176471 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.08235294 -0.05098039 -0.00392157 ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2894668/1065248138.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_inputs['combined'] = pd.Series(test_inputs.apply(expand_and_concatenate, axis=1))\n",
      "/tmp/ipykernel_2894668/1065248138.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submit_inputs['combined'] = pd.Series(submit_inputs.apply(expand_and_concatenate, axis=1))\n"
     ]
    }
   ],
   "source": [
    "def expand_and_concatenate(row):\n",
    "    level_9 = row['level_9']\n",
    "    onehot = row['OneHot']\n",
    "    \n",
    "    # Repeat the onehot array to match the number of samples in level_9\n",
    "    expanded_onehot = np.tile(onehot, (level_9.shape[0], 1))\n",
    "    \n",
    "    # Concatenate the level_9 and expanded onehot arrays along the last dimension\n",
    "    concatenated_array = np.concatenate((level_9, expanded_onehot), axis=1)\n",
    "    \n",
    "    return concatenated_array\n",
    "\n",
    "# Apply the function to each row and store the result in a new column\n",
    "train_inputs['combined'] = pd.Series(train_inputs.apply(expand_and_concatenate, axis=1))\n",
    "test_inputs['combined'] = pd.Series(test_inputs.apply(expand_and_concatenate, axis=1))\n",
    "submit_inputs['combined'] = pd.Series(submit_inputs.apply(expand_and_concatenate, axis=1))\n",
    "\n",
    "# Verify the shape of the combined array\n",
    "print(train_inputs.iloc[0]['combined'].shape)  # Should print (3722, 178)\n",
    "print(test_inputs.iloc[0]['combined'].shape)\n",
    "print(submit_inputs.iloc[0]['combined'].shape)  # Should print (3722, 178)\n",
    "\n",
    "# Print the combined array for verification\n",
    "print(train_inputs['combined'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f306d3e-a96d-46c1-9a19-a0599a9197f5",
   "metadata": {},
   "source": [
    "# NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f0bf77-d4e8-4b9d-9e48-291f1f999f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6416962, 34)\n",
      "(6416962, 20)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numpy.linalg import solve\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NTK:\n",
    "    def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "        self.reg = reg\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.sol = None\n",
    "        self.Xtrain = None\n",
    "        self.ytrain = None\n",
    "\n",
    "    def ntk_kernel(self, pair1, pair2):\n",
    "        out = pair1 @ pair2.T + 1\n",
    "        N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "        N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "        XX = np.sqrt(N1 @ N2.T)\n",
    "        out = out / XX\n",
    "\n",
    "        out = np.clip(out, -1, 1)\n",
    "\n",
    "        first = (\n",
    "            1\n",
    "            / np.pi\n",
    "            * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "            * XX\n",
    "        )\n",
    "        sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "        out = first + sec\n",
    "\n",
    "        C = 1\n",
    "        return out / C\n",
    "\n",
    "    def gaussian_kernel(self, pair1, pair2):\n",
    "        sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "        return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "    def kernel(self, pair1, pair2):\n",
    "        ntk = self.ntk_kernel(pair1, pair2)\n",
    "        gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "        return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "    def fit(self, Xtrain, ytrain, batch_size=1000):\n",
    "        num_samples = Xtrain.shape[0]\n",
    "        self.Xtrain = Xtrain\n",
    "        self.ytrain = ytrain\n",
    "        \n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        self.indices = indices\n",
    "        \n",
    "        subset_indices = indices[:batch_size]\n",
    "        Xtrain_subset = Xtrain[subset_indices]\n",
    "        ytrain_subset = ytrain[subset_indices]\n",
    "        \n",
    "        K = self.kernel(Xtrain_subset, Xtrain_subset)\n",
    "        self.sol = solve(K + self.reg * np.eye(len(K)), ytrain_subset).T\n",
    "        \n",
    "        self.Xtrain_subset = Xtrain_subset\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=1000):\n",
    "        num_samples = X.shape[0]\n",
    "        K = np.zeros((self.Xtrain_subset.shape[0], num_samples))\n",
    "        \n",
    "        for i in range(0, self.Xtrain_subset.shape[0], batch_size):\n",
    "            for j in tqdm.tqdm(range(0, num_samples, batch_size)):\n",
    "                K[i:i+batch_size, j:j+batch_size] = self.kernel(self.Xtrain_subset[i:i+batch_size], X[j:j+batch_size])\n",
    "        \n",
    "        return (self.sol @ K).T\n",
    "\n",
    "\n",
    "train_input1 = np.vstack(train_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "train_out1 = np.vstack(train_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "\n",
    "\n",
    "print(train_input1.shape)\n",
    "print(train_out1.shape)\n",
    "\n",
    "model1 = NTK(reg=0.01, alpha=1.0)\n",
    "# Fit model and predict using batched training\n",
    "model1 = model1.fit(train_input1, train_out1, batch_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08ef4a-f7ee-41ee-8fef-a37c5e64f626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6417/6417 [10:20<00:00, 10.34it/s]\n",
      "100%|██████████| 6417/6417 [10:21<00:00, 10.33it/s]\n",
      "100%|██████████| 6417/6417 [10:00<00:00, 10.69it/s]\n",
      "100%|██████████| 6417/6417 [10:31<00:00, 10.16it/s]\n",
      "100%|██████████| 6417/6417 [10:58<00:00,  9.75it/s]\n",
      " 67%|██████▋   | 4325/6417 [08:08<03:50,  9.07it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train_predict1 = model1.predict(train_input1)\n",
    "test_predict1 = model1.predict(test_input1)\n",
    "train_r21 = r2_score(train_out1, train_predict1)\n",
    "test_r21 = r2_score(test_out1, test_predict1)\n",
    "print(f\"Train_2 R2:  {train_r21:.4f}, Test_2 R2:  {test_r21:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3af9c3f5-bc1d-4a7d-83be-3003f6cea88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6416962, 178)\n",
      "(6416962, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 4982/6417 [1:04:30<18:34,  1.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m model1 \u001b[38;5;241m=\u001b[39m NTK(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Fit model and predict using batched training\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_out1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[1;32m    110\u001b[0m train_predict1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mpredict(train_input1)\n",
      "Cell \u001b[0;32mIn[192], line 74\u001b[0m, in \u001b[0;36mNTK.fit\u001b[0;34m(self, Xtrain, ytrain, batch_size, epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msol \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msol, sol])\n\u001b[0;32m---> 74\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXtrain_subset \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtrain_subset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/AiTLS/lib/python3.10/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from numpy.linalg import solve\n",
    "import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NTK:\n",
    "    def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "        self.reg = reg\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.sol = None\n",
    "        self.Xtrain = None\n",
    "        self.ytrain = None\n",
    "\n",
    "    def ntk_kernel(self, pair1, pair2):\n",
    "        out = pair1 @ pair2.T + 1\n",
    "        N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "        N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "        XX = np.sqrt(N1 @ N2.T)\n",
    "        out = out / XX\n",
    "\n",
    "        out = np.clip(out, -1, 1)\n",
    "\n",
    "        first = (\n",
    "            1\n",
    "            / np.pi\n",
    "            * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "            * XX\n",
    "        )\n",
    "        sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "        out = first + sec\n",
    "\n",
    "        C = 1\n",
    "        return out / C\n",
    "\n",
    "    def gaussian_kernel(self, pair1, pair2):\n",
    "        sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "        return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "    def kernel(self, pair1, pair2):\n",
    "        ntk = self.ntk_kernel(pair1, pair2)\n",
    "        gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "        return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "    def fit(self, Xtrain, ytrain, batch_size=1000, epochs=10):\n",
    "        import tqdm\n",
    "        num_samples = Xtrain.shape[0]\n",
    "        self.Xtrain = Xtrain\n",
    "        self.ytrain = ytrain\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for i in tqdm.tqdm(range(0, num_samples, batch_size)):\n",
    "                subset_indices = indices[i:i+batch_size]\n",
    "                Xtrain_subset = Xtrain[subset_indices]\n",
    "                ytrain_subset = ytrain[subset_indices]\n",
    "                \n",
    "                K = self.kernel(Xtrain_subset, Xtrain_subset)\n",
    "                sol = solve(K + self.reg * np.eye(len(K)), ytrain_subset).T\n",
    "                \n",
    "                if self.sol is None:\n",
    "                    self.sol = sol\n",
    "                    self.Xtrain_subset = Xtrain_subset\n",
    "                else:\n",
    "                    self.sol = np.vstack([self.sol, sol])\n",
    "                    self.Xtrain_subset = np.vstack([self.Xtrain_subset, Xtrain_subset])\n",
    "                    \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] completed\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=1000):\n",
    "        num_samples = X.shape[0]\n",
    "        num_train_samples = self.Xtrain_subset.shape[0]\n",
    "        K = np.zeros((num_train_samples, num_samples))\n",
    "        \n",
    "        for i in tqdm.tqdm(range(0, num_train_samples, batch_size)):\n",
    "            for j in range(0, num_samples, batch_size):\n",
    "                K[i:i+batch_size, j:j+batch_size] = self.kernel(self.Xtrain_subset[i:i+batch_size], X[j:j+batch_size])\n",
    "        \n",
    "        return (self.sol @ K).T\n",
    "\n",
    "# # Example data preparation\n",
    "# train_inputs = pd.DataFrame({\n",
    "#     'combined': [np.random.randn(3722, 178) for _ in range(3)]\n",
    "# })\n",
    "# train_outputs = pd.DataFrame({\n",
    "#     'level_9': [np.random.randn(3722, 20) for _ in range(3)]\n",
    "# })\n",
    "\n",
    "# train_input1 = np.vstack(train_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "# train_out1 = np.vstack(train_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "\n",
    "print(train_input1.shape)\n",
    "print(train_out1.shape)\n",
    "\n",
    "model1 = NTK(reg=0.01, alpha=1.0)\n",
    "# Fit model and predict using batched training\n",
    "model1 = model1.fit(train_input1, train_out1, batch_size=1000, epochs=1)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train_predict1 = model1.predict(train_input1)\n",
    "test_predict1 = model1.predict(test_input1)\n",
    "train_r21 = r2_score(train_out1, train_predict1)\n",
    "test_r21 = r2_score(test_out1, test_predict1)\n",
    "print(f\"Train R2: {train_r21:.4f}\")\n",
    "print(f\"Test R2: {test_r21:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "de461d64-5454-4968-8b96-ae39f2e58532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X, batch_size=1000):\n",
    "    num_samples = X.shape[0]\n",
    "    num_train_samples = self.Xtrain_subset.shape[0]\n",
    "    K = np.zeros((num_train_samples, num_samples))\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, num_train_samples, batch_size)):\n",
    "        for j in range(0, num_samples, batch_size):\n",
    "            K[i:i+batch_size, j:j+batch_size] = self.kernel(self.Xtrain_subset[i:i+batch_size], X[j:j+batch_size])\n",
    "    \n",
    "    return (self.sol @ K).T\n",
    "\n",
    "model1.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "686657e8-e004-407e-9584-30bb1a498336",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[0;32m----> 3\u001b[0m train_predict1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_predict1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mpredict(test_input1)\n\u001b[1;32m      5\u001b[0m train_r21 \u001b[38;5;241m=\u001b[39m r2_score(train_out1, train_predict1)\n",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train_predict1 = model1.predict(train_input1)\n",
    "test_predict1 = model1.predict(test_input1)\n",
    "train_r21 = r2_score(train_out1, train_predict1)\n",
    "test_r21 = r2_score(test_out1, test_predict1)\n",
    "print(f\"Train R2: {train_r21:.4f}\")\n",
    "print(f\"Test R2: {test_r21:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "00a0d4e6-29ea-4ef7-aa9e-3e928c17fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model2(model1):\n",
    "    @staticmethod\n",
    "    def predict(self, X, batch_size=1000):\n",
    "        num_samples = X.shape[0]\n",
    "        num_train_samples = self.Xtrain_subset.shape[0]\n",
    "        K = np.zeros((num_train_samples, num_samples))\n",
    "        \n",
    "        for i in tqdm.tqdm(range(0, num_train_samples, batch_size)):\n",
    "            for j in range(0, num_samples, batch_size):\n",
    "                K[i:i+batch_size, j:j+batch_size] = self.kernel(self.Xtrain_subset[i:i+batch_size], X[j:j+batch_size])\n",
    "        \n",
    "        return (self.sol @ K).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "01bb53e2-1919-4d28-9147-335eca577b68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NTK' object has no attribute 'Xtrain_subset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_predict1 = model2.predict(train_input1)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_predict1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# train_r21 = r2_score(train_out1, train_predict1)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m test_r21 \u001b[38;5;241m=\u001b[39m r2_score(test_out1, test_predict1)\n",
      "Cell \u001b[0;32mIn[192], line 82\u001b[0m, in \u001b[0;36mNTK.predict\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m     81\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 82\u001b[0m     K \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXtrain_subset\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], num_samples))\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXtrain_subset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size):\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_samples, batch_size)):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NTK' object has no attribute 'Xtrain_subset'"
     ]
    }
   ],
   "source": [
    "# train_predict1 = model2.predict(train_input1)\n",
    "test_predict1 = model2.predict(test_input1)\n",
    "# train_r21 = r2_score(train_out1, train_predict1)\n",
    "test_r21 = r2_score(test_out1, test_predict1)\n",
    "# print(f\"Train R2: {train_r21:.4f}\")\n",
    "print(f\"Test R2: {test_r21:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d4f91-8f63-4611-a33e-17d963810551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from numpy.linalg import solve\n",
    "import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NTK:\n",
    "    def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "        self.reg = reg\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.sol = None\n",
    "        self.Xtrain_subset = None\n",
    "        self.ytrain_subset = None\n",
    "\n",
    "    def ntk_kernel(self, pair1, pair2):\n",
    "        out = pair1 @ pair2.T + 1\n",
    "        N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "        N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "        XX = np.sqrt(N1 @ N2.T)\n",
    "        out = out / XX\n",
    "\n",
    "        out = np.clip(out, -1, 1)\n",
    "\n",
    "        first = (\n",
    "            1\n",
    "            / np.pi\n",
    "            * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "            * XX\n",
    "        )\n",
    "        sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "        out = first + sec\n",
    "\n",
    "        C = 1\n",
    "        return out / C\n",
    "\n",
    "    def gaussian_kernel(self, pair1, pair2):\n",
    "        sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "        return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "    def kernel(self, pair1, pair2):\n",
    "        ntk = self.ntk_kernel(pair1, pair2)\n",
    "        gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "        return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "    def fit(self, Xtrain, ytrain, batch_size=1000, epochs=10):\n",
    "        num_samples = Xtrain.shape[0]\n",
    "        self.Xtrain = Xtrain\n",
    "        self.ytrain = ytrain\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for i in tqdm.tqdm(range(0, num_samples, batch_size)):\n",
    "                subset_indices = indices[i:i+batch_size]\n",
    "                Xtrain_subset = Xtrain[subset_indices]\n",
    "                ytrain_subset = ytrain[subset_indices]\n",
    "                \n",
    "                K = self.kernel(Xtrain_subset, Xtrain_subset)\n",
    "                sol = solve(K + self.reg * np.eye(len(K)), ytrain_subset).T\n",
    "                \n",
    "                if self.sol is None:\n",
    "                    self.sol = sol\n",
    "                    self.Xtrain_subset = Xtrain_subset\n",
    "                else:\n",
    "                    self.sol = np.vstack([self.sol, sol])\n",
    "                    self.Xtrain_subset = np.vstack([self.Xtrain_subset, Xtrain_subset])\n",
    "                    \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] completed\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=1000):\n",
    "        num_samples = X.shape[0]\n",
    "        num_train_samples = self.Xtrain_subset.shape[0]\n",
    "        K = np.zeros((num_train_samples, num_samples))\n",
    "        \n",
    "        for i in tqdm.tqdm(range(0, num_train_samples, batch_size)):\n",
    "            for j in range(0, num_samples, batch_size):\n",
    "                K[i:i+batch_size, j:j+batch_size] = self.kernel(self.Xtrain_subset[i:i+batch_size], X[j:j+batch_size])\n",
    "        \n",
    "        return (self.sol @ K).T\n",
    "\n",
    "print(train_input1.shape)\n",
    "print(train_out1.shape)\n",
    "\n",
    "model1 = NTK(reg=0.01, alpha=1.0)\n",
    "# Fit model and predict using batched training\n",
    "model1 = model1.fit(train_input1, train_out1, batch_size=1000, epochs=1)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train_predict1 = model1.predict(train_input1)\n",
    "test_predict1 = model1.predict(test_input1)\n",
    "train_r21 = r2_score(train_out1, train_predict1)\n",
    "test_r21 = r2_score(test_out1, test_predict1)\n",
    "print(f\"Train R2: {train_r21:.4f}\")\n",
    "print(f\"Test R2: {test_r21:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ae789c0a-2624-49cb-ba12-aced73252577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from numpy.linalg import solve\n",
    "\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# class NTK:\n",
    "#     def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "#         super().__init__()\n",
    "#         self.reg = reg\n",
    "#         self.sigma = sigma\n",
    "#         self.alpha = alpha\n",
    "#         self.sol = None\n",
    "#         self.Xtrain = None\n",
    "\n",
    "#     def ntk_kernel(self, pair1, pair2):\n",
    "#         out = pair1 @ pair2.T + 1\n",
    "#         N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "#         N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "#         XX = np.sqrt(N1 @ N2.T)\n",
    "#         out = out / XX\n",
    "\n",
    "#         out = np.clip(out, -1, 1)\n",
    "\n",
    "#         first = (\n",
    "#             1\n",
    "#             / np.pi\n",
    "#             * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "#             * XX\n",
    "#         )\n",
    "#         sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "#         out = first + sec\n",
    "\n",
    "#         C = 1\n",
    "#         return out / C\n",
    "\n",
    "#     def gaussian_kernel(self, pair1, pair2):\n",
    "#         sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "#         return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "#     def kernel(self, pair1, pair2):\n",
    "#         ntk = self.ntk_kernel(pair1, pair2)\n",
    "#         gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "#         return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "#     def fit(self, Xtrain, ytrain):\n",
    "#         K = self.kernel(Xtrain, Xtrain)\n",
    "#         sol = solve(K + self.reg * np.eye(len(K)), ytrain).T\n",
    "#         self.sol = sol\n",
    "#         self.Xtrain = Xtrain\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         K = self.kernel(self.Xtrain, X)\n",
    "#         return (self.sol @ K).T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from numpy.linalg import solve\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NTK:\n",
    "    def __init__(self, reg=1, sigma=1.0, alpha=0.5):\n",
    "        self.reg = reg\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.sol = None\n",
    "        self.Xtrain = None\n",
    "\n",
    "    def ntk_kernel(self, pair1, pair2):\n",
    "        out = pair1 @ pair2.T + 1\n",
    "        N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "        N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "        XX = np.sqrt(N1 @ N2.T)\n",
    "        out = out / XX\n",
    "\n",
    "        out = np.clip(out, -1, 1)\n",
    "\n",
    "        first = (\n",
    "            1\n",
    "            / np.pi\n",
    "            * (out * (np.pi - np.arccos(out)) + np.sqrt(1.0 - np.power(out, 2)))\n",
    "            * XX\n",
    "        )\n",
    "        sec = 1 / np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "        out = first + sec\n",
    "\n",
    "        C = 1\n",
    "        return out / C\n",
    "\n",
    "    def gaussian_kernel(self, pair1, pair2):\n",
    "        sq_dist = np.sum(pair1**2, axis=1).reshape(-1, 1) + np.sum(pair2**2, axis=1) - 2 * np.dot(pair1, pair2.T)\n",
    "        return np.exp(-sq_dist / (2 * self.sigma**2))\n",
    "\n",
    "    def kernel(self, pair1, pair2):\n",
    "        ntk = self.ntk_kernel(pair1, pair2)\n",
    "        gaussian = self.gaussian_kernel(pair1, pair2)\n",
    "        return self.alpha * ntk + (1 - self.alpha) * gaussian\n",
    "\n",
    "    def fit(self, Xtrain, ytrain, batch_size=1000):\n",
    "        import tqdm\n",
    "        num_samples = Xtrain.shape[0]\n",
    "        self.Xtrain = Xtrain\n",
    "        \n",
    "        K = np.zeros((num_samples, num_samples))\n",
    "        \n",
    "        for i in tqdm.tqdm(range(0, num_samples, batch_size)):\n",
    "            for j in range(0, num_samples, batch_size):\n",
    "                K[i:i+batch_size, j:j+batch_size] = self.kernel(Xtrain[i:i+batch_size], Xtrain[j:j+batch_size])\n",
    "        \n",
    "        self.sol = solve(K + self.reg * np.eye(len(K)), ytrain).T\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=1000):\n",
    "        import tqdm\n",
    "        num_samples = X.shape[0]\n",
    "        K = np.zeros((self.Xtrain.shape[0], num_samples))\n",
    "        \n",
    "        for i in tqdm.tqdm(range(0, self.Xtrain.shape[0], batch_size)):\n",
    "            for j in range(0, num_samples, batch_size):\n",
    "                K[i:i+batch_size, j:j+batch_size] = self.kernel(self.Xtrain[i:i+batch_size], X[j:j+batch_size])\n",
    "        \n",
    "        return (self.sol @ K).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "48e9e576-97d3-47da-a97a-5573a37d9730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6416962, 178)\n",
      "(6416962, 20)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 300. TiB for an array with shape (6416962, 6416962) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model1 \u001b[38;5;241m=\u001b[39m NTK(reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Fit model and predict using batched training\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_out1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 113\u001b[0m, in \u001b[0;36mNTK.fit\u001b[0;34m(self, Xtrain, ytrain, batch_size)\u001b[0m\n\u001b[1;32m    110\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m Xtrain\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXtrain \u001b[38;5;241m=\u001b[39m Xtrain\n\u001b[0;32m--> 113\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_samples, batch_size)):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_samples, batch_size):\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 300. TiB for an array with shape (6416962, 6416962) and data type float64"
     ]
    }
   ],
   "source": [
    "train_input1 = np.vstack(train_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "train_out1 = np.vstack(train_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "test_input1 = np.vstack(test_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "test_out1 = np.vstack(test_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "\n",
    "print(train_input1.shape)\n",
    "print(train_out1.shape)\n",
    "\n",
    "model1 = NTK(reg=0.01, alpha=1.0)\n",
    "# Fit model and predict using batched training\n",
    "model1 = model1.fit(train_input1, train_out1, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88a92c-ccb5-4b95-97d7-9b8f731114f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f06730-edb9-4dcd-b738-b2f590a9b3eb",
   "metadata": {},
   "source": [
    "# KAN w/Fourier instead of b-spline (Chat-GPT + https://github.com/GistNoesis/FourierKAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325a373-3e51-4368-8647-d6dd48961664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6416962, 34)\n",
      "(6416962, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38891/38891 [02:12<00:00, 294.07it/s]\n",
      "100%|██████████| 6417/6417 [00:50<00:00, 125.94it/s]\n",
      "100%|██████████| 2541/2541 [00:19<00:00, 130.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 32869.7925, Val Loss: 0.8677, Train R2: -25.6559, Val R2: -24.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38891/38891 [02:12<00:00, 293.53it/s]\n",
      "100%|██████████| 38891/38891 [02:10<00:00, 297.78it/s]\n",
      "100%|██████████| 38891/38891 [02:12<00:00, 292.92it/s]\n",
      "100%|██████████| 38891/38891 [02:14<00:00, 289.73it/s]\n",
      "100%|██████████| 38891/38891 [02:12<00:00, 294.49it/s]\n",
      "100%|██████████| 38891/38891 [02:15<00:00, 286.73it/s]\n",
      "100%|██████████| 38891/38891 [02:17<00:00, 283.47it/s]\n",
      "100%|██████████| 38891/38891 [02:20<00:00, 277.08it/s]\n",
      "100%|██████████| 38891/38891 [02:14<00:00, 288.86it/s]\n",
      "100%|██████████| 38891/38891 [02:19<00:00, 279.35it/s]\n",
      "100%|██████████| 6417/6417 [00:48<00:00, 133.17it/s]\n",
      "100%|██████████| 2541/2541 [00:18<00:00, 139.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 32873.8759, Val Loss: 0.8407, Train R2: -27.9149, Val R2: -27.4229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 18589/38891 [01:03<01:12, 281.09it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 38891/38891 [02:17<00:00, 282.28it/s]\n",
      " 23%|██▎       | 8985/38891 [00:32<01:59, 250.19it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class FourierKANLinear(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, gridsize, addbias=True, smooth_initialization=False):\n",
    "        super(FourierKANLinear, self).__init__()\n",
    "        self.gridsize = gridsize\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        \n",
    "        grid_norm_factor = (torch.arange(gridsize) + 1)**2 if smooth_initialization else np.sqrt(gridsize)\n",
    "        \n",
    "        self.fouriercoeffs = nn.Parameter(torch.randn(2, outdim, inputdim, gridsize) / \n",
    "                                          (np.sqrt(inputdim) * grid_norm_factor))\n",
    "        if self.addbias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xshp = x.shape\n",
    "        outshape = xshp[0:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "\n",
    "        k = torch.reshape(torch.arange(1, self.gridsize + 1, device=x.device), (1, 1, 1, self.gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        \n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        \n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2], (-2, -1))\n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "        \n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y\n",
    "\n",
    "class FourierKAN(nn.Module):\n",
    "    def __init__(self, layers_hidden, gridsize, addbias=True, smooth_initialization=False):\n",
    "        super(FourierKAN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
    "            self.layers.append(FourierKANLinear(in_features, out_features, gridsize, addbias, smooth_initialization))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, learning_rate=0.0001, epochs=100, return_losses=False):\n",
    "    import tqdm\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in tqdm.tqdm(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss += criterion(val_outputs, val_targets).item()\n",
    "            val_loss /= len(val_loader)\n",
    "            losses_val.append(val_loss)\n",
    "\n",
    "            if (epoch % 10 == 0) or (epoch == epochs - 1):\n",
    "                train_r2, _ = evaluate_model(model, train_loader.dataset.tensors[0].cpu().numpy(), train_loader.dataset.tensors[1].cpu().numpy())\n",
    "                val_r2, _ = evaluate_model(model, val_loader.dataset.tensors[0].cpu().numpy(), val_loader.dataset.tensors[1].cpu().numpy())\n",
    "                print(f'Epoch [{epoch}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}')\n",
    "\n",
    "    if return_losses:\n",
    "        return losses, losses_val\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def evaluate_model(model, input_data, target_data=None, batch_size=1000):\n",
    "    import tqdm\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(torch.tensor(input_data).float(), torch.tensor(target_data).float()) if target_data is not None else TensorDataset(torch.tensor(input_data).float())\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            if target_data is not None:\n",
    "                inputs, targets = batch\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                all_targets.append(targets.cpu().numpy())\n",
    "            else:\n",
    "                inputs = batch[0]\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "    \n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "    \n",
    "    if target_data is not None:\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        return r2_score(all_targets, all_outputs), all_outputs\n",
    "    else:\n",
    "        return all_outputs\n",
    "\n",
    "# Prepare data for training\n",
    "def prepare_dataloader(train_input, train_output, test_input, test_output, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(train_input).float(), torch.tensor(train_output).float())\n",
    "    val_dataset = TensorDataset(torch.tensor(test_input).float(), torch.tensor(test_output).float())\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_input1 = np.vstack(train_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "train_out1 = np.vstack(train_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "test_input1 = np.vstack(test_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "test_out1 = np.vstack(test_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "\n",
    "print(train_input1.shape)\n",
    "print(train_out1.shape)\n",
    "\n",
    "# Prepare DataLoader\n",
    "batch_size = 165\n",
    "train_loader, val_loader = prepare_dataloader(train_input1, train_out1, test_input1, test_out1, batch_size)\n",
    "\n",
    "# Model definition\n",
    "input_size = train_input1.shape[1]\n",
    "output_size = train_out1.shape[1]\n",
    "layers_hidden = [input_size, 10, 10, output_size]\n",
    "gridsize = 15\n",
    "\n",
    "model1 = FourierKAN(layers_hidden, gridsize, smooth_initialization=True)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "train_model(model1, train_loader, val_loader, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "# Evaluation\n",
    "train_r2, train_predict1 = evaluate_model(model1, train_input1, train_out1)\n",
    "test_r2, test_predict1 = evaluate_model(model1, test_input1, test_out1)\n",
    "\n",
    "print(f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "eac0819c-91b9-4837-b357-479c7426650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38891 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "model1 = model1.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "losses_val = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model1.eval()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in tqdm.tqdm(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model1(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "65424802-4016-4459-9819-c17e93ed282c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1995, -0.2335,  2.1997,  ...,  0.2881, -0.1221,  0.1318],\n",
       "        [ 1.0030, -0.3096,  0.2798,  ...,  0.9457,  1.8414,  2.0253],\n",
       "        [ 0.1408,  0.9058,  1.3564,  ...,  0.2161,  0.3684,  0.4066],\n",
       "        ...,\n",
       "        [-0.2018, -0.9146,  1.1333,  ..., -0.0602,  0.1246, -0.0916],\n",
       "        [ 0.7301, -0.0739, -0.6566,  ..., -0.2857,  0.2411,  0.0655],\n",
       "        [-0.1189, -0.7444,  1.0237,  ...,  0.1279,  1.0676,  1.7605]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d8ef3f01-244d-4018-a9fe-56d4200b0f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8690, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4f812-274e-4317-94a7-2a0ff69735de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2830e265-4c55-405b-a921-c9a2c4b28b5e",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fa561106-eb01-43e4-94c2-a197e6a62f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/msc/home/vsharm64/miniconda3/envs/AiTLS/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.42it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 39.92it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 40.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 70.4119, Val Loss: 1.0009, Train R2: -0.0015, Val R2: -0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.54it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.57it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.62it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.50it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.54it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.99it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.48it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.41it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.43it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.45it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 38.25it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 39.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 68.6323, Val Loss: 1.0012, Train R2: -0.0018, Val R2: -0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.51it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.54it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.42it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.22it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.36it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.45it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.48it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.49it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.38it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.30it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 34.72it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 68.4738, Val Loss: 1.0018, Train R2: -0.0024, Val R2: -0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.38it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.27it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.37it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.29it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.40it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.41it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.80it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.47it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.27it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 36.63it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 38.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Loss: 68.3454, Val Loss: 1.0004, Train R2: -0.0015, Val R2: -0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.37it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.63it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.37it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.81it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.35it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.36it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.30it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.49it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.27it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 39.72it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 39.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 68.2641, Val Loss: 1.0014, Train R2: -0.0018, Val R2: -0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.29it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.90it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.44it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.45it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.55it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.36it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.37it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.87it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.31it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 39.56it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 38.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 68.1786, Val Loss: 1.0003, Train R2: -0.0007, Val R2: -0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.39it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.47it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.42it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.93it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.30it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.45it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.58it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.34it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 38.29it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Loss: 68.1327, Val Loss: 1.0003, Train R2: -0.0014, Val R2: -0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.52it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.46it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.64it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.91it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.44it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.45it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.41it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.41it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.45it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 38.26it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 39.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Loss: 68.1284, Val Loss: 1.0010, Train R2: -0.0014, Val R2: -0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.24it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.97it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.38it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.35it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.50it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.31it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.36it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.88it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.34it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 39.44it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 37.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Loss: 68.1368, Val Loss: 1.0011, Train R2: -0.0014, Val R2: -0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.44it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.34it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.27it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.31it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.41it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.50it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 11.79it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.43it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.45it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 37.50it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 38.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Loss: 68.0977, Val Loss: 1.0001, Train R2: -0.0008, Val R2: -0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:05<00:00, 12.31it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.32it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.40it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.00it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.35it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.36it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.42it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.33it/s]\n",
      "100%|██████████| 68/68 [00:05<00:00, 12.29it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 37.99it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 40.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: -0.0011, Test R2: -0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, inputdim, outdim, num_layers=4, nhead=4, dim_feedforward=128, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.input_projection = nn.Linear(inputdim, dim_feedforward)\n",
    "        self.input_norm = nn.LayerNorm(dim_feedforward)\n",
    "        self.positional_encoding = nn.Parameter(self._generate_positional_encoding(1, dim_feedforward), requires_grad=False)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=dim_feedforward, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        self.output_norm = nn.LayerNorm(dim_feedforward)\n",
    "\n",
    "        self.decoder = nn.Linear(dim_feedforward, outdim)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_seq_len, dim_feedforward):\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim_feedforward, 2) * -(np.log(10000.0) / dim_feedforward))\n",
    "        pe = torch.zeros(max_seq_len, dim_feedforward)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x).unsqueeze(1)  # Add a sequence length dimension\n",
    "        x = self.input_norm(x)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:, :seq_len]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_norm(x)\n",
    "        x = self.decoder(x).squeeze(1)  # Remove the sequence length dimension\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, val_loader, learning_rate=0.0001, epochs=100, return_losses=False):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in tqdm.tqdm(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss += criterion(val_outputs, val_targets).item()\n",
    "            val_loss /= len(val_loader)\n",
    "            losses_val.append(val_loss)\n",
    "\n",
    "            if (epoch % 10 == 0) or (epoch == epochs - 1):\n",
    "                train_r2, _ = evaluate_model(model, train_loader.dataset.tensors[0].cpu().numpy(), train_loader.dataset.tensors[1].cpu().numpy())\n",
    "                val_r2, _ = evaluate_model(model, val_loader.dataset.tensors[0].cpu().numpy(), val_loader.dataset.tensors[1].cpu().numpy())\n",
    "                print(f'Epoch [{epoch}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}')\n",
    "\n",
    "    if return_losses:\n",
    "        return losses, losses_val\n",
    "\n",
    "def evaluate_model(model, input_data, target_data=None, batch_size=1000):\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(torch.tensor(input_data).float(), torch.tensor(target_data).float()) if target_data is not None else TensorDataset(torch.tensor(input_data).float())\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            if target_data is not None:\n",
    "                inputs, targets = batch\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                all_targets.append(targets.cpu().numpy())\n",
    "            else:\n",
    "                inputs = batch[0]\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "    \n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "    \n",
    "    if target_data is not None:\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        return r2_score(all_targets, all_outputs), all_outputs\n",
    "    else:\n",
    "        return all_outputs\n",
    "\n",
    "def prepare_dataloader(train_input, train_output, test_input, test_output, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(train_input).float(), torch.tensor(train_output).float())\n",
    "    val_dataset = TensorDataset(torch.tensor(test_input).float(), torch.tensor(test_output).float())\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_input1 = np.vstack(train_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "train_out1 = np.vstack(train_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "test_input1 = np.vstack(test_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "test_out1 = np.vstack(test_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "\n",
    "print(train_input1.shape)\n",
    "\n",
    "# Model definition\n",
    "input_size = train_input1.shape[1]\n",
    "output_size = train_out1.shape[1]\n",
    "num_layers = 50\n",
    "nhead = 8\n",
    "dim_feedforward = 256\n",
    "dropout = 0.1\n",
    "\n",
    "model1 = TransformerModel(input_size, output_size, num_layers, nhead, dim_feedforward, dropout)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.0001\n",
    "epochs = 100\n",
    "\n",
    "train_model(model1, train_loader, val_loader, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "# Evaluation\n",
    "train_r2, train_predict1 = evaluate_model(model1, train_input1, train_out1)\n",
    "test_r2, test_predict1 = evaluate_model(model1, test_input1, test_out1)\n",
    "\n",
    "print(f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e97fdc3e-7af4-4213-be25-47948af2bc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11166, 20)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "96378079-19ed-4437-8ad6-cb6638c7aea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "input_size = train_input1.shape[1]\n",
    "output_size = train_out1.shape[1]\n",
    "num_layers = 50\n",
    "nhead = 8\n",
    "dim_feedforward = 256\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerModel(input_size, output_size, num_layers, nhead, dim_feedforward, dropout)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "losses = []\n",
    "losses_val = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in tqdm.tqdm(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if return_losses:\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        losses_val.append(val_loss)\n",
    "\n",
    "        if (epoch % 10 == 0) or (epoch == epochs - 1):\n",
    "            train_r2, _ = evaluate_model(model, train_loader.dataset.tensors[0].cpu().numpy(), train_loader.dataset.tensors[1].cpu().numpy())\n",
    "            val_r2, _ = evaluate_model(model, val_loader.dataset.tensors[0].cpu().numpy(), val_loader.dataset.tensors[1].cpu().numpy())\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}')\n",
    "\n",
    "if return_losses:\n",
    "    return losses, losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1459b556-e010-4555-af71-fd313e4b9137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([165, 178])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f8c4e57b-e58b-4601-bc05-bdaf3a0d384d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([165, 20])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e976ac29-bc06-4211-9d47-8f79e162c8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([165, 20])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ce3d6-4e69-4f39-9eb3-8e36648fa6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47a61f4-c8bb-45d3-a18a-8d2b39cda790",
   "metadata": {},
   "source": [
    "# Conditional Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5e09c77e-4e65-4ebb-9d78-814def1aed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/msc/home/vsharm64/miniconda3/envs/AiTLS/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Original Inputs Shape: (11166, 8)\n",
      "Train Conditionals Shape: (11166, 170)\n",
      "Train Outputs Shape: (11166, 20)\n",
      "Test Original Inputs Shape: (11166, 8)\n",
      "Test Conditionals Shape: (11166, 170)\n",
      "Test Outputs Shape: (11166, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/175 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 175\u001b[0m\n\u001b[1;32m    172\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m    173\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 175\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m    178\u001b[0m train_r2, train_predict1 \u001b[38;5;241m=\u001b[39m evaluate_model(model_T, train_original_inputs, train_conditionals, train_out1)\n",
      "Cell \u001b[0;32mIn[219], line 59\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, learning_rate, epochs, return_losses)\u001b[0m\n\u001b[1;32m     57\u001b[0m src, extra, tgt \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mto(device), extra\u001b[38;5;241m.\u001b[39mto(device), tgt\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, tgt)\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/AiTLS/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/AiTLS/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[219], line 33\u001b[0m, in \u001b[0;36mConditionalTransformer.forward\u001b[0;34m(self, src, extra, tgt)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Concatenate extra features with the input\u001b[39;00m\n\u001b[1;32m     32\u001b[0m extra_expanded \u001b[38;5;241m=\u001b[39m extra\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, src_seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_expanded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m src_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_embedding(src) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:, :src_seq_len, :]\n\u001b[1;32m     36\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embedding(tgt) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding[:, :tgt_seq_len, :]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ConditionalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, extra_dim, output_dim, d_model=256, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=512, dropout=0.1):\n",
    "        super(ConditionalTransformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.extra_dim = extra_dim\n",
    "        self.d_model = d_model\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.encoder_embedding = nn.Linear(input_dim + extra_dim, d_model)\n",
    "        self.decoder_embedding = nn.Linear(output_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 5000, d_model))\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, extra, tgt):\n",
    "        src_seq_len, tgt_seq_len = src.shape[1], tgt.shape[1]\n",
    "\n",
    "        # Concatenate extra features with the input\n",
    "        extra_expanded = extra.unsqueeze(1).expand(-1, src_seq_len, -1)\n",
    "        src = torch.cat((src, extra_expanded), dim=-1)\n",
    "\n",
    "        src_emb = self.encoder_embedding(src) + self.positional_encoding[:, :src_seq_len, :]\n",
    "        tgt_emb = self.decoder_embedding(tgt) + self.positional_encoding[:, :tgt_seq_len, :]\n",
    "\n",
    "        src_emb = src_emb.permute(1, 0, 2)\n",
    "        tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "\n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        output = self.fc_out(output.permute(1, 0, 2))\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train_model(model, train_loader, val_loader, learning_rate=0.0001, epochs=100, return_losses=False):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for src, extra, tgt in tqdm.tqdm(train_loader):\n",
    "            src, extra, tgt = src.to(device), extra.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, extra, tgt)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if return_losses:\n",
    "            losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for val_src, val_extra, val_tgt in val_loader:\n",
    "                    val_src, val_extra, val_tgt = val_src.to(device), val_extra.to(device), val_tgt.to(device)\n",
    "                    val_outputs = model(val_src, val_extra, val_tgt)\n",
    "                    val_loss += criterion(val_outputs, val_tgt).item()\n",
    "            val_loss /= len(val_loader)\n",
    "            losses_val.append(val_loss)\n",
    "\n",
    "            if (epoch % 100 == 0) or (epoch == epochs - 1):\n",
    "                train_r2, _ = evaluate_model(model, train_loader.dataset.tensors[0].cpu().numpy(), train_loader.dataset.tensors[1].cpu().numpy(), train_loader.dataset.tensors[2].cpu().numpy())\n",
    "                val_r2, _ = evaluate_model(model, val_loader.dataset.tensors[0].cpu().numpy(), val_loader.dataset.tensors[1].cpu().numpy(), val_loader.dataset.tensors[2].cpu().numpy())\n",
    "                print(f'Epoch [{epoch}/{epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}')\n",
    "\n",
    "    if return_losses:\n",
    "        return losses, losses_val\n",
    "\n",
    "def evaluate_model(model, input_data, extra_data, target_data=None, batch_size=1000):\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(torch.tensor(input_data).float(), torch.tensor(extra_data).float(), torch.tensor(target_data).float()) if target_data is not None else TensorDataset(torch.tensor(input_data).float(), torch.tensor(extra_data).float())\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            if target_data is not None:\n",
    "                src, extra, targets = batch\n",
    "                src, extra, targets = src.to(device), extra.to(device), targets.to(device)\n",
    "                outputs = model(src, extra, targets)\n",
    "                all_targets.append(targets.cpu().numpy())\n",
    "            else:\n",
    "                src, extra = batch\n",
    "                src, extra = src.to(device), extra.to(device)\n",
    "                outputs = model(src, extra, torch.zeros_like(src))\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "    \n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "    \n",
    "    if target_data is not None:\n",
    "        all_targets = np.vstack(all_targets)\n",
    "        return r2_score(all_targets, all_outputs), all_outputs\n",
    "    else:\n",
    "        return all_outputs\n",
    "\n",
    "def prepare_dataloader(original_inputs, conditionals, outputs, batch_size, shuffle=True):\n",
    "    dataset = TensorDataset(torch.tensor(original_inputs).float(), torch.tensor(conditionals).float(), torch.tensor(outputs).float())\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "# Extract the original input (first 8 dimensions) and the 170 one-hot encoded variables\n",
    "def extract_inputs_and_conditionals(data):\n",
    "    original_inputs = data[:, :8]\n",
    "    conditionals = data[:, 8:]\n",
    "    return original_inputs, conditionals\n",
    "\n",
    "# Example data preparation\n",
    "train_inputs = pd.DataFrame({\n",
    "    'combined': [np.random.randn(3722, 178) for _ in range(3)]\n",
    "})\n",
    "train_outputs = pd.DataFrame({\n",
    "    'level_9': [np.random.randn(3722, 20) for _ in range(3)]\n",
    "})\n",
    "test_inputs = pd.DataFrame({\n",
    "    'combined': [np.random.randn(3722, 178) for _ in range(3)]\n",
    "})\n",
    "test_outputs = pd.DataFrame({\n",
    "    'level_9': [np.random.randn(3722, 20) for _ in range(3)]\n",
    "})\n",
    "\n",
    "train_input1 = np.vstack(train_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "train_out1 = np.vstack(train_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "test_input1 = np.vstack(test_inputs['combined']) #metadata_df.iloc[0]['level_9']\n",
    "test_out1 = np.vstack(test_outputs['level_9']) #metadata_df.iloc[2]['level_9']\n",
    "\n",
    "# Extract the original inputs and conditionals\n",
    "train_original_inputs, train_conditionals = extract_inputs_and_conditionals(train_input1)\n",
    "test_original_inputs, test_conditionals = extract_inputs_and_conditionals(test_input1)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Train Original Inputs Shape: {train_original_inputs.shape}\")\n",
    "print(f\"Train Conditionals Shape: {train_conditionals.shape}\")\n",
    "print(f\"Train Outputs Shape: {train_out1.shape}\")\n",
    "print(f\"Test Original Inputs Shape: {test_original_inputs.shape}\")\n",
    "print(f\"Test Conditionals Shape: {test_conditionals.shape}\")\n",
    "print(f\"Test Outputs Shape: {test_out1.shape}\")\n",
    "\n",
    "# Prepare DataLoader\n",
    "batch_size = 64\n",
    "train_loader = prepare_dataloader(train_original_inputs, train_conditionals, train_out1, batch_size)\n",
    "val_loader = prepare_dataloader(test_original_inputs, test_conditionals, test_out1, batch_size, shuffle=False)\n",
    "\n",
    "# Model definition\n",
    "input_size = train_original_inputs.shape[1]\n",
    "extra_size = train_conditionals.shape[1]\n",
    "output_size = train_out1.shape[1]\n",
    "\n",
    "model_T = ConditionalTransformer(input_dim=input_size, extra_dim=extra_size, output_dim=output_size)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "epochs = 2\n",
    "\n",
    "train_model(model_T, train_loader, val_loader, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "# Evaluation\n",
    "train_r2, train_predict1 = evaluate_model(model_T, train_original_inputs, train_conditionals, train_out1)\n",
    "test_r2, test_predict1 = evaluate_model(model_T, test_original_inputs, test_conditionals, test_out1)\n",
    "\n",
    "print(f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ae5c1-bea2-441e-8d81-6a7e41425f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AiTLS",
   "language": "python",
   "name": "aitls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
